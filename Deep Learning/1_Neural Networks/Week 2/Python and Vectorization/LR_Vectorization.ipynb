{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Vectorization**: \n",
    "Method of utilizing functions that do faster numerical computation using parallel processing capabilities of CPU or GPU than for loops. \n",
    "In python numpy methods are vectorized, always perform better than for loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.random.rand(1000000)\n",
    "b = np.random.rand(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized time = 0.7810592651367188 milli secs\n",
      "250355.004922\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "z = np.dot(a,b)\n",
    "end = time.time()\n",
    "print(\"Vectorized time = {0}{1}\".format(1000*(end -start),' milli secs'))\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Loop time = 346.8492031097412 milli secs\n",
      "250355.004922\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "z = 0\n",
    "for i in range(1000000):\n",
    "    z+= a[i]*b[i]\n",
    "end = time.time()\n",
    "print(\"For Loop time = {0}{1}\".format(1000*(end -start),' milli secs'))\n",
    "print(z)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]]\n",
      "[[0]\n",
      " [1]\n",
      " [2]\n",
      " [3]]\n",
      "[[ 0  2  4  6]\n",
      " [ 4  6  8 10]\n",
      " [ 8 10 12 14]]\n"
     ]
    }
   ],
   "source": [
    "# Vectorize \n",
    "a = np.arange(12).reshape(3,4)\n",
    "b = np.arange(4).reshape(4,1)\n",
    "c = np.zeros_like(a)\n",
    "\n",
    "# a.shape = (3,4)\n",
    "# b.shape = (4,1)\n",
    "\n",
    "for i in range(3):\n",
    "  for j in range(4):\n",
    "    c[i][j] = a[i][j] + b[j]\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  2,  4,  6],\n",
       "       [ 4,  6,  8, 10],\n",
       "       [ 8, 10, 12, 14]])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance of Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Deep Learning, we will mostly deal with massive calculations, vectorization can make a big difference in completing\n",
    "a single iteration. \n",
    "On smaller data sets, it used to be 'good to have' option, in deep learning, it becomes must have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy methods perform vectorization and are way faster than loops. \n",
    "Example: element wise operations performed by functions-\n",
    "exp(x), abs(x), exp(x) , log(x), 1/x where x is a matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization of Logistic Regression using representation in 1_LR_algo_math_rep.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to compute $z^{(i)}, a^{(i)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def Z(X, W, b):\n",
    "    '''\n",
    "    Argumnts : \n",
    "    X : n * m matrix , every feature is a row\n",
    "    W : n*1 column vector\n",
    "    b : a scalar\n",
    "    \n",
    "    Returns:\n",
    "    z : 1*m matrix'''\n",
    "    #print(\"dimension of X is {0}, W is {1}, B is {2}\".format(str(X.shape), str(W.shape),str(B.shape)))\n",
    "    z = np.dot(W.T,X)+ b  # use of broadcasting\n",
    "    return(z)\n",
    "\n",
    "def A(z):\n",
    "    '''\n",
    "    Arguments:\n",
    "    z : 1*m matrix\n",
    "    Returns:\n",
    "    a : 1*m matrix\n",
    "    '''\n",
    "    a = 1/(1 + np.exp(-1*z))\n",
    "    #print(\"dimension of a matrix are {0}\".format(str(a.shape)))\n",
    "    return(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to compute Cost Function J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Log_Loss(y_vec, a_vec):\n",
    "    '''\n",
    "    Arguments:\n",
    "    y_vec: 1*m vector\n",
    "    a_vec: 1*m vector\n",
    "    Returns:\n",
    "    loss : a scalar quantity'''\n",
    "    #print(\"dimension of a matrices y and a are {0} and {1}\".format(str(y_vec.shape),str(a_vec.shape)))\n",
    "    m = y_vec.shape[1]\n",
    "    loss = -1 *(y_vec * np.log(a_vec) + (1-y_vec)*(np.log(1-a_vec)))\n",
    "    cost = (1/m) *np.sum(loss, axis = 1)\n",
    "    return cost    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to update weights/ implement one step of gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_desc(X, w, a_vec, y_vec, b, alpha):\n",
    "    m = y_vec.shape[0]\n",
    "    '''\n",
    "    The line for vector dw_vec below computes dw_j for all j in a single matrix multiplication\n",
    "    \n",
    "    Arguments:\n",
    "    X: n*m matrix\n",
    "    w: n*1 vector\n",
    "    a_vec: 1*m vector\n",
    "    y_vec: 1*m vector\n",
    "    b : scalar\n",
    "    alpha: learning rate, a scalar\n",
    "    \n",
    "    Returns:\n",
    "    w_new : n*1 vector of updated weights\n",
    "    b_new : scalar\n",
    "    '''\n",
    "    dz_vec = a_vec-y_vec\n",
    "    dw_vec = np.dot(X, dz_vec.T) # n*1 vec\n",
    "    db = np.sum(dz_vec, axis = 1)\n",
    "    w_new =  w - (alpha/m)* dw_vec\n",
    "    b_new = b - (alpha/m)*db\n",
    "    \n",
    "    return w_new,b_new\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Implementation to train a logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### Logistic Regression Algorithm #############\n",
    "#1. Randomly initialize weights, bias\n",
    "#2. Compute z\n",
    "#3. Compute a\n",
    "#4. Compute Cost\n",
    "#5. Adjust weights and bias\n",
    "#6. Compute cost\n",
    "#7. Check if cost of step 5 decreases than in step 3, if yes go to step 2, if no decrease happened for 10 iterations\n",
    "#  stop, and return cost, weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_train(seed, iters, X, y, alpha):\n",
    "    '''\n",
    "       Arguments:\n",
    "       X     : n*m matrix of numerical features\n",
    "       y     : 1*m vector of binary labels\n",
    "       iter  : max. no of iterations if exit criteria is not satisfied, to avoid long training iteration,\n",
    "              switch to using larger learning rate\n",
    "       alpha : learning rate  \n",
    "       \n",
    "       Returns:\n",
    "       Function trains a logistic regression models, returns:\n",
    "       1. Weight and bias vector corresponding to the least cost model\n",
    "       2. Least cost\n",
    "       3. Full cost vector for analysis of progression of costs\n",
    "    '''\n",
    "    np.random.seed(seed)\n",
    "    Costs = []\n",
    "    Weights = []\n",
    "    Bias = []\n",
    "    cost_inc = 0\n",
    "    dim = X.shape\n",
    "    #1. Initialize random weights as a column vector with elements equal to features (n*1)\n",
    "    w_init = np.random.rand(dim[0],1)\n",
    "    assert(w_init.shape == (dim[0],1))\n",
    "    #1. Initialize random bias as a column vector with elements equal to records in feature matrix (m*1)\n",
    "    b_init = np.random.rand()\n",
    "    #2. Compute z\n",
    "    z_init = Z(X,w_init,b_init)\n",
    "    #3. Compute a\n",
    "    a_init = A(z_init)\n",
    "    #4. Compute initial Cost\n",
    "    Cost_init = Log_Loss(y,a_init)\n",
    "    Cost = 0\n",
    "    for i in range(iters):\n",
    "        if (i ==0):\n",
    "            w = w_init\n",
    "            b = b_init\n",
    "            z = z_init\n",
    "            a = a_init\n",
    "            Cost = Cost_init\n",
    "        #print(w,\"\\n\")\n",
    "        #print(Cost,\"\\n\")    \n",
    "        #5. Adjust weights and bias\n",
    "        w,b = grad_desc(X = X, w = w, a_vec = a, y_vec = y, b = b, alpha = alpha)\n",
    "        Weights.append(w)\n",
    "        Bias.append(b)\n",
    "        #6. Compute new cost\n",
    "        z = Z(X,w,b)\n",
    "        a = A(z)\n",
    "        Cost_new= Log_Loss(y,a)\n",
    "        Costs.append(Cost_new)\n",
    "        if (Cost_new > Cost):\n",
    "            cost_inc +=1\n",
    "        if ((Cost_new > Cost) & cost_inc > 5):\n",
    "            min_cost = min(Costs)\n",
    "            ind = Costs.index(min_cost)\n",
    "            return Weights[ind],Bias[ind],Costs[ind], Costs\n",
    "        Cost = Cost_new\n",
    "    min_cost = min(Costs)\n",
    "    ind = Costs.index(min_cost)  \n",
    "    return Weights[ind],Bias[ind],Costs[ind], Costs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on sample data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data set of 3 features, 5 records\n",
    "years = [1,4,3,10,5]\n",
    "rating_5yr_avg = [2,1,3,4,5]\n",
    "salary_incr_5yr_avg = [5, 2.5, 3.5, 4, 6]\n",
    "churn = [1,1,0,1,0]\n",
    "\n",
    "# Feature matrix x, with a column representing a single feature with m  = 5 records\n",
    "X = np.array([years, rating_5yr_avg, salary_incr_5yr_avg])\n",
    "assert(X.shape == (3,5))\n",
    "dim = X.shape\n",
    "y = np.array(churn).reshape(1,dim[1])  # A row vector requires reshape to convert from rank 1 array to proper array\n",
    "seed = 101\n",
    "Weight_best,Bias_best,Cost_best, Costs = logistic_train(seed, 10000,X,y,0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3.22152126],\n",
       "       [-11.45107998],\n",
       "       [  5.32748433]])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Weight_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.20978417])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bias_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00074572])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Cost_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x\n",
    "pred_z = Z(X, Weight_best,Bias_best)\n",
    "pred_a = A(pred_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9.98826979e-01,   9.99999517e-01,   1.92238360e-03,\n",
       "          9.99453290e-01,   8.33255688e-05]])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 1, 0]])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
