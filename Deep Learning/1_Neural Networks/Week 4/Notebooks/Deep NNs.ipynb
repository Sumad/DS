{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Deep vs Shallow Neural Networks \n",
    "### 2. Why Deep Networks  \n",
    "### 3. Notation for deep networks \n",
    "### 4. Forward Propagation  \n",
    "### 5. Getting Dimensions right  \n",
    "### 6. Building Blocks - Implementation of Forward and Back Propagation in Deep L NN  \n",
    "### 7. Generic equations for Deep L NN  to be implemented as functions  \n",
    "### 8. Parameters vs Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Deep vs Shallow Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shallowness of network is defined in terms of the number of layers it has. Width may be good to refer to no. of hiddern units in a layer. \n",
    "A shallow neural networks is one like a simple logistic regression, with one hidden layer and an output layer.  \n",
    "So, more the number of hidden layers, more deep a networks is.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Why Deep Networks  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has been shown that layeres in deep networks are able to learn incrementally more and more complex features. The deeper layers build on features from shallower layers.   \n",
    "So, deep networks have been able to learn features that have been difficult to learn for other type of models.  \n",
    "Eamples of Deep NN use areas:  \n",
    "1. Face Recognition :  \n",
    "- When shallow layers were used to reconstruct images, it was seen they learn features like edges of images. \n",
    "- Deeper layers, **composed** on these features to learn facial features, deepest layers further composed to learn \n",
    "  faces.\n",
    "2. Audio to text.  \n",
    "- Similar patterns observed,shallow layers learned phonemes (character level sounds), deeper layers can learn words, and deepest sentences  \n",
    "##### Why not use shallower networks than deep layered networks**   \n",
    "- From circuit theory it was show that to learn  functions like XOR b/w n inputs, the number of units required\n",
    "in 'small/less wide' but deep network are of the order logn whereas for shallow (and wide) network are of the order $2^n$  \n",
    "- For n = 100, comes to be 6 vs 2^100.\n",
    "\n",
    "Example of Deep NN : \n",
    "- 4 Layers \n",
    "- with 3 hidden layers, having 5,5,3 hidden units. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../images/IMG_0026.JPG' style = \"width:600px; height:400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Notation for deep networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. No. of Layers. - L , capital L denotes the last layer as well, as seen above\n",
    "2. No. of hidden units in a layer.  \n",
    "$n^{l}$. \n",
    "3. Input features matrix.  \n",
    "$A^{l} = X$\n",
    "4. Activation vector of a layer.  \n",
    "$A^{l}$  \n",
    "5. Weight Matrix and bias vector.  \n",
    "$W^{l}, b^{l}$  \n",
    "6. Activation functions for a layer.  \n",
    "$g^{[l]}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Forward Propagation  \n",
    "- Vectorized Equations of the above Deep L NN for  m examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align} \n",
    "A^{[0]} &= X  \\tag1 \\\\\n",
    "Z^{[1]}  &= {W^{[1]}} . A^{[0]} + b^{[l]} \\tag 2\\\\\n",
    "A^{[1]} &= g^{[1]}(Z^{[1]})\\tag 3\\\\\n",
    "Z^{[2]}  &= {W^{[2]}} . A^{[1]} + b^{[2]} \\tag 4\\\\\n",
    "A^{[2]} &= g^{[2]}(Z^{[2]}) \\tag 5 \\\\\n",
    "Z^{[3]}  &= {W^{[3]}} . A^{[2]} + b^{[3]} \\tag 6\\\\\n",
    "A^{[3]} &= g^{[3]}(Z^{[3]}) \\tag 7 \\\\ \n",
    "Z^{[4]}  &= {W^{[4]}} . A^{[3]} + b^{[4]} \\tag 8\\\\\n",
    "A^{[4]} &= Y =  g^{[4]}(Z^{[4]}) \\tag 9 \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Getting Dimensions right, for a Deep L NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensions for parameters:\n",
    "1. W - $n^{[l]} * n^{[l-1]}$\n",
    "2. b - $n^{[l]}$\n",
    "\n",
    "Dimensions of activations:  \n",
    "1. z , Z - $n^{[l]} * 1$ and $n^{[l]} * m$ \n",
    "2. a , A - $n^{[l]} * 1$ and $n^{[l]} * m$\n",
    "\n",
    "Dimensions of I/P and O/P ( for classification setting):  \n",
    "x, X -  $n^{[0]} * 1$ and $n^{[0]} * m$  \n",
    "y, Y - $1*n^{[L]}$ and $1*n^{[L]} * m$\n",
    "\n",
    "Dimension of gradients: \n",
    "dZ, dA, db are sames as Z,A,b resp.   \n",
    "\n",
    "**With this, the matrix multiploications in equations 1-9 above can be checked**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Building Blocks - Implementation of Forward and Back Propagation in Deep L NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs and Outputs produced by each layer can be visualized in the figure below, for fwd and back prop. flows\n",
    "<img src='../images/IMG_0027.JPG' style = \"width:600px; height:400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the forward and backward propagation through the network above, inputs and outputs of the blocks can be summarized as below  \n",
    "##### Forward Prop  \n",
    "- Input for first layer - X. \n",
    "- Each layer can be stated to have the following Inputs, Outputs in matrix form. \n",
    "  - Input for layer l = ${{A^{[l-1]}}, W^{[l]}}, {b^{[1]}}$  \n",
    "  - Outputs for layer l = $ Z^{[l]},  A^{[l]}$  \n",
    "-   A generic function can be implemented (representing the empty boxes in the flow of fwd. prop. to take inputs as above, and produde the outputs.   \n",
    "\n",
    "##### Backward Prop   \n",
    "- Initial Input for backward prop needs to be computed, **for logistic loss function it is **\n",
    "  \\begin{align}\n",
    "  dA^{[l]} = \\text{ for i in 1 to m } \\frac{{A^{[L]}}^{(i)} - Y^{[i]}}{{A^{[L]}}^{(i)}(1 - {a^{[L]}}^{(i)})}\n",
    "  \\end{align}\n",
    "- Inputs for layer l : ${dA^{[l]}}, W^{[l]}, b^{[l]}, Z^{[l]}$  \n",
    "- Intermediate output : $dZ^{[l]}$  \n",
    "- Output for next layer : $dA^{[l-1]}$\n",
    "- Output for updating weight, bais of the layer : $dW^{[l]}, db^{[l]}$   \n",
    "\n",
    "##### Need for caching. \n",
    "Outputs from forward propagation are used in backward propgation steps, so they need to be cached. In think big matices need to be stored till their use is complete, hence the concept of cache. \n",
    "$W^{[l]}, b^{[l]}, Z^{[l]}$ need to be cached for each layer until back prop. picks them for use\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Generic equations for Deep L NN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 example case "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Forward Prop.  Outputs\n",
    "\\begin{align}  \n",
    "z^{[l]}  &= {W^{[l]}} . a^{[l]} + b^{[l]}  \\\\\n",
    "a^{[l]} &=  g^{[l]}(z^{[l]})           \\\\ \n",
    "\\end{align}  \n",
    "\n",
    "##### Backward Prop.  Outputs\n",
    "Assuming $g{[1]}$ and $g{[2]}$ are sigmoid functions, we'll get:   \n",
    "###### Intermediate Output. \n",
    "\\begin{align}\n",
    "dz^{[l]} &= {g^{[l]}}^{'}(z^{[l]})* da^{[l]}\\\\\n",
    "\\end{align}  \n",
    "###### Output for next layer  \n",
    "\\begin{align}\n",
    "da^{[l-1]} &= {W^{[l]}}^T. dz^{[l]}\\\\\n",
    "\\end{align}  \n",
    "\n",
    "###### Output for updating wieights, bias of the layer\n",
    "\\begin{align}\n",
    "dW^{[l]} &=  dz^{[l]}.{z^{[l-1]}}^T  \\\\\n",
    "db^{[l]} &=  dz^{[l]} \\\\\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### m example case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Forward Prop.  Outputs\n",
    "\\begin{align}  \n",
    "Z^{[l]}  &= {W^{[l]}} . A^{[l]} + b^{[l]}  \\\\\n",
    "A^{[l]} &=  g^{[l]}(Z^{[l]})           \\\\\n",
    "\\end{align}  \n",
    "\n",
    "##### Backward Prop.  Outputs\n",
    "Assuming $g{[1]}$ and $g{[2]}$ are sigmoid functions, we'll get:   \n",
    "###### Intermediate Output. \n",
    "\\begin{align}\n",
    "dZ^{[l]} &= {g^{[l]}}^{'}(Z^{[l]})* dA^{[l]}\\\\\n",
    "\\end{align}  \n",
    "###### Output for next layer  \n",
    "\\begin{align}\n",
    "dA^{[l-1]} &= {W^{[l]}}^T. dZ^{[l]}\\\\\n",
    "\\end{align}  \n",
    "\n",
    "###### Output for updating wieights, bias of the layer\n",
    "\\begin{align}\n",
    "dW^{[l]} &=  \\frac{1}{m} dZ^{[l]}.{A^{[l-1]}}^T  \\\\\n",
    "db^{[l]} &= \\frac{1}{m}\\; np.sum(dZ^{[l]}, \\;axis = 1, \\; \\text{keepdims = True)} \\\\\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Points :**  \n",
    "- Each activation has a different derivative. Thus, during backpropagation you need to know which activation was used in the forward propagation to\n",
    "be able to compute the correct derivative  \n",
    "- Forward propagation propagates the input through the layers, we can vectorize computations for all units in a layer\n",
    "  but we cannoot prevent use of for loops to go through each layer for fwd. and backward propagation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.Parameters vs Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameters are setting in the algorithm which govern the final learned values of parameters.** Typically, \n",
    "a range of hyperparameters are used in deep learning.   \n",
    "A broad classification can be -  \n",
    "1. Architectural hyperparameters.   \n",
    "  - Number of layers, no. of hidden units, activation functions, further like m from leaky relu   \n",
    "2. Learning rate.  \n",
    "3. Length of iterations.  \n",
    "4. Regularization. \n",
    "4. Others to be covered. (possibly related to the learning algo. like gradient descent, mini-batch grad descent)\n",
    "  - Momentum, mini-batch size   \n",
    "  \n",
    "Hyperparameter search to find values that provide good parameters is an empirical process. \n",
    "- idea, experiment, evaluate, and repeat. \n",
    "\n",
    "**Hyperparameters could change with data / time, and there is a need to check for best hyperparameters.  \n",
    " Good advice is to search for hyperparameters when starting with a new line of work, and not necessarily carry  \n",
    " them over from known areas of work**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
