{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Representation - Example of 3 input features, 1 hidden layer with 4 units for a binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"../images/2layer_NN.jpg\" style = \"width:600px; height:400px;\">\n",
    "Input layer is not counted as a layer, this network i called 2 layer NN.\n",
    "Every input feature from input layer links to hidden unit of hidden layer, and every unit from hidden layer links to \n",
    "output layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Hidden layer as a combination of multiple logistic units & Representation of matrices corresponding to a layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single unit in a hidden layer can be viewed as a logistic unit discussed in week 1 and 2. A layer then becomes a collection of such logistic units, each taking feed from all the inputs from previous layer. The matrix representations for one hidden layer can be described as\n",
    "\n",
    "**a Matrix**  \n",
    "$a^{[k]}$ matrix represents the activation of kth layer, and specifically. \n",
    "* $a^{[0]} = X$ will be input layer, n x m order matrix, so 3 x m order here  \n",
    "* $a^{[1]}$ will be hidden layer , of order h x 1, h being no. of hidden units , here 4 x 1 represented as  \n",
    "$\\left\\lgroup \\matrix {a^{[1]}_1 \\cr a^{[1]}_2 \\cr a^{[1]}_3 \\cr a^{[1]}_4} \\right \\rgroup$  \n",
    "* $a^{[2]} = \\widehat y $ will be output layer , of 1 x 1 dimention here  \n",
    "\n",
    "**z matrix**   \n",
    "$z^{[k]}$ matrix serves as input to derive activation, and $a^{[k]} = g(z^{[k]})$ specifically - .  \n",
    "  * $z^{[1]}$ will be for hidden layer , of order h x 1, h being no. of hidden units , here 4 x 1  \n",
    "    $\\left\\lgroup \\matrix {z^{[1]}_1 \\cr z^{[1]}_2 \\cr z^{[1]}_3 \\cr z^{[1]}_4} \\right \\rgroup$ \n",
    "  * $z^{[2]}$ will be for output layer , of 1 x 1 dimention here \n",
    "\n",
    "\n",
    "** Parameter Matrices**  \n",
    "\n",
    "**Weight Matrix **  \n",
    "\n",
    "Weight vectors are defined with respect to a unit in a layer. For a unit in layer k, fed from h units from layer k-1,\n",
    "it will have h weights, and 1 bias.In general - \n",
    "* **Weight vector** defined for a unit j in layer k is $w^{[k]}_j$, with dimensions h x 1, h being units from k-1 layer feeding this unit. \n",
    "* For network above there are 4 weight vectors for each hidden unit in layer 1 - $w^{[1]}_1 \\;, w^{[1]}_2,\\; w^{[1]}_3,\\; w^{[1]}_4$  , with a vector expanded as  \n",
    "$w^{[1]}_1 = \\left\\lgroup \\matrix {w^{[1]}_{11} \\cr w^{[1]}_{12} \\cr w^{[1]}_{13}}  \\right \\rgroup$  \n",
    "* The **Weight matrix ** for this layer can be defined by stacking these weight vectors in cols as below. The first position in subscript identifies the hidden unit in layer, the second identifies the hidden unit in previous layer connecting to it  \n",
    "$W^{[1]} = \\left\\lgroup \\matrix {w^{[1]}_{11} & w^{[1]}_{21} & w^{[1]}_{31} & w^{[1]}_{41} \\cr w^{[1]}_{12} & w^{[1]}_{22} & w^{[1]}_{32} & w^{[1]}_{42} \\cr w^{[1]}_{13} & w^{[1]}_{23} & w^{[1]}_{33} & w^{[1]}_{43}} \\right \\rgroup$\n",
    "\n",
    "\n",
    "**Bias Matrix**\n",
    "\n",
    "There is one bias scalar for one hidden unit, so for a layer of h hidden units, the bias matrix is a column  \n",
    "vector with dims h x 1. Example for layer 1:  \n",
    "$ b^{[1]} = \\left\\lgroup \\matrix{b^{[1]}_1 \\cr b^{[1]}_2 \\cr b^{[1]}_3 \\cr b^{[1]}_4}\\right\\rgroup$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Vectorizing computation of Y hat for  using 1 example for full 2 layer network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"../images/unit_computation.jpg\" style = \"width:400px; height: 300px\">\n",
    "\n",
    "Computation for a single unit in the layer is in two parts as shown above for  $z^{[1]} \\text{and} \\; a^{[1]} \\; \\text {using} W^{[1]}$ \n",
    "Computing $\\widehat y = a^{[2]}$ is achieved using following forward propagation steps:  \n",
    "**Layer 1**   \n",
    "* $ z^{[1]}  =\n",
    "\\left\\lgroup \\matrix{z^{[1]}_1 \\cr z^{[1]}_2 \\cr z^{[1]}_3 \\cr z^{[1]}_4}\\right\\rgroup = \n",
    "\\left\\lgroup \\matrix {w^{[1]}_{11} & w^{[1]}_{21} & w^{[1]}_{31} & w^{[1]}_{41} \\cr w^{[1]}_{12} & w^{[1]}_{22} & w^{[1]}_{32} & w^{[1]}_{42} \\cr w^{[1]}_{13} & w^{[1]}_{23} & w^{[1]}_{33} & w^{[1]}_{43}} \\right \\rgroup^T .\n",
    "\\left\\lgroup \\matrix{x_1 \\cr x_2 \\cr x_3}\\right \\rgroup + \n",
    "\\left\\lgroup \\matrix{b^{[1]}_1 \\cr b^{[1]}_2 \\cr b^{[1]}_3 \\cr b^{[1]}_4}\\right\\rgroup$  \n",
    "* $a^{[1]} = \\left\\lgroup \\matrix{a^{[1]}_1 \\cr a^{[1]}_2 \\cr a^{[1]}_3 \\cr a^{[1]}_4}\\right\\rgroup = \n",
    "\\sigma (z^{[1]})$  \n",
    "\n",
    "\n",
    "\n",
    "**Layer 2** \n",
    "* $z^{[2]} = z^{[2]}_1 =  \n",
    "\\left\\lgroup \\matrix {w^{[2]}_{11} \\cr w^{[2]}_{12} \\cr w^{[2]}_{13} \\cr w^{[2]}_{14}} \\right \\rgroup^T \\left\\lgroup \\matrix {a^{[1]}_1 \\cr a^{[1]}_2 \\cr a^{[1]}_3 \\cr a^{[1]}_4} \\right \\rgroup + b^{[2]}_1$  \n",
    "\n",
    "* $a^{[2]} = \\sigma (z^{[2]})$\n",
    "\n",
    "\n",
    "The above computations can be vectorized for 1 example case as above using matrices as defined above, we do not need to use for loops to calculate each element for Z vector in layers 1 and 2. \n",
    "A concize matrix representation can be written as :  \n",
    " \n",
    "\\begin{align}  \n",
    "z^{[1]}  &= {W^{[1]}}^T . x + b^{[1]} \\tag 1\\\\\n",
    "a^{[1]} &= \\sigma (z^{[1]})\\tag 2\\\\\n",
    "z^{[2]}  &= {W^{[2]}}^T . a^{[1]} + b^{[2]} \\tag 3\\\\\n",
    "a^{[2]} &= \\sigma (z^{[2]}) \\tag 4\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** The dimensions of pramter matrices depend on the architecture of the network only, the no. of training examples affect the dimensions of Z and A matrices.\n",
    "In multiclass classfication problem, the output classes will also affect Z, A matrices**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Vectorizing computation of Y hat using m examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 4 equations for case of single example can be more specifically written as. - \n",
    "\\begin{align}  \n",
    "z^{[1](i)}  &= {W^{[1]}}^T . x^{(i)} + b^{[1]} \\tag 1\\\\\n",
    "a^{[1](i)} &= \\sigma (z^{[1](i)})\\tag 2\\\\\n",
    "z^{[2](i)}  &= {W^{[2]}}^T . a^{[1](i)} + b^{[2]} \\tag 3\\\\\n",
    "a^{[2](i)} &= \\sigma (z^{[2](i)}) \\tag 4\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate over m examples, we can use a for loop, but a vectorized implementation would be as -  \n",
    "\\begin{align}\n",
    "Z^{[1]}  &= {W^{[1]}}^T . X + b^{[1]} \\tag 4\\\\\n",
    "A^{[1]} &= \\sigma (Z^{[1]})\\tag 5\\\\\n",
    "Z^{[2]}  &= {W^{[2]}}^T . A^{[1]} + b^{[2]} \\tag 6\\\\\n",
    "A^{[2]} &= \\sigma (Z^{[2]}) \\tag 7\n",
    "\\end{align}  \n",
    "\n",
    "**Note capitalized X,Z,A**  \n",
    "* Eq 4 : X - dims are 3 x m, $W^{[1]}$ - dims are 3 x 4, dims of $b^{[1]}$ are 4 x 1 broadcasted to 4 x m, so $Z^{[1]}$ has dims 4 x m  \n",
    "* Eq 5 : $A^{[1]}$ has dims 4 x m  \n",
    "* Eq 6 : $W^{[2]}$ - dims are 4 x 1,dims of $b^{[2]}$ are 1 x 1 broadcasted to 1 x m, so $Z^{[2]}$ has dims 1 x m  \n",
    "* Eq 7 : $A^{[2]}$ has dims 1 x m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** An element of matrix Z or A is represented as $Z^{[k]}_{i,\\;j} \\; , A^{[k]}_{i,\\;j}$** where (i,j) represent dims of matrices, k represents layer number -    \n",
    "k - represents layer. (0,1,2,...)  \n",
    "i - represents the hidden unit numbers (1,2...)  \n",
    "j - represents the example (1,2,... m)  \n",
    "\n",
    " ** Takeaways** -  \n",
    " - $A^{[k]}_{i,\\;j}$ represents activation on example j by hiddent unit i in layer k  \n",
    " - All neurons in a hidden layer generate an activation on a training example, obtained as a column vector, represented as $ z_{,j} = g(W^T . X_{,j})$. So, $ z_{,1}, z_{,2}, .., z_{,m} $ vectors are stacked together for all training examples to yield Z matrix, same can be visualized for A matrix \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1 Why use an activation function  (Non -linear function applied to z). \n",
    "1. Identity Functions do not yield features to get good predictive performance. \n",
    "2. To generate non-linear features, in a neural network, with hidden layers, one has to apply a non-linear transfromation, else the output simply becomes a linear transformation of inputs (from a composition of linear inputs)   A good example from the above 2 layer network. \n",
    "\\begin{align}\n",
    "z^{[1](i)}  &= {W^{[1]}}^T . x^{(i)} + b^{[1]} \\\\\n",
    "a^{[1](i)} &= 1* (z^{[1](i)})\\\\\n",
    "z^{[2](i)}  &= {W^{[2]}}^T . a^{[1](i)} + b^{[2]} \\\\\n",
    "a^{[2](i)} &= 1 * (z^{[2](i)})  \\\\ \n",
    "a^{[2](i)} &= {W^{[2]}}^T . ({W^{[1]}}^T . x^{(i)} + b^{[1]}) + b^{[2]} \\tag 5\\\\\n",
    "a^{[2](i)} &= W^{'}. x^{(i)} + b^{'} \\tag 6\n",
    "\\end{align}  \n",
    "\n",
    "** Identity Function**. \n",
    "Can be a choice for output function if problem is a regression problem. Even then we might find useful to choose others -\n",
    "Example : ReLU to ensure non-negative outputs for house price prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2 Choice of activation functions, popular ones \n",
    "### Sigmoid, Tanh, Relu (Rectified Linear Unit) , Leaky Relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaky ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.3 Pros and Cons, recommendations to make a choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
