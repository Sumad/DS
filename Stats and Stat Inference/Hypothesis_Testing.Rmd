---
title: "Hypothesis Testing"
author: "Sumad Singh"
date: "September 25, 2017"
output: github_document
---

```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)
```
## HYPOTHESIS TESTING
## TYPES OF HYPOTHESIS TESTS WHEN COMPARING POPULATIONS (TBDe)
COMPARE MEASURES OF CONTINUOUS DATA LIKE MEANS,PROPORTIONS,VARIANCES - Z,T,F ;  
COMPARE MEASURES OF CATEGORICAL DATA LIKE COUNTS - CHISQUARE (TBDe)
UNDER CONDITIONS OF POPULATION PARAMETER KNOWN/UNKNOWN, SAMPLE SIZE LARGE/SMALL
UNDERLYING DISTRIBUTIONS KNOWN/UNKNOWN
## P VALUE, DECISION ERRORS, POWER OF A TEST
## ANNOVA - ONE WAY, TWO WAY (TBD)
## NON PARAMETRIC TESTS USED FOR CONTINUOUS MEASURES WHEN ASSUMOPTIONS ARE VIOLATED (TBD)
### MANN WHITNEU U TEST
### WILCOXON T TEST
### KRUSHAL WALIS H TEST

NOTE : TBDE MEAN TO BE DETAILED ,TBD TO BE DONE


### HYPOTHESIS TESTING
Problem Solving in Business context requires testing beleifs, prevelant opinions about a population
of interest to introduce interventions.
Example: 
- White lit bulbs over the shelves can imrpove sales than day lit bulbs.  
- Customers would not defect if given an increase in premium equal or less than inflation rate  
The case for interventions is made by hypothesis testing, it involves-  
1. Defining the business problem  
2. Laying out Hypothesis tests  
3. Identifying data (sample) to be gathered for testing hypothesis,  
   sampling considerations ( random, stratified etc), size etc  
4. Identifying test statistic, confidence interval/significance level/ testing error considerations  
5. Testing,Derive inferences and make recommendations  

#### 1. HYPOTHESIS STATEMENT
A positive assertion about a **population characteristic** based on a currently held beleif.  
- Null Hypothesis: captures it mathematically by using relevant metrics. Null Hypothesis is stated  
using an **inequality (<= or >=)**.  
- Alternate hypothsis: states a counter beleif mathematically.  

**Any problem statement, or a change to an existing norm/well accepted idea that needs to be proved is put 
as 'Alternate Hypothesis'. Unless proven, null hypothesis is the accepted idea.** 

Examples: 
PROBLEM STATEMENT-  
A a new driver is more likely to suffer a loss in first 6 months/first policy period  
than subsequent policy periods, so 'no. of times renewed' could be a predictor for  
ultimate loss  

Null Hypothesis: Mean loss of new drivers in 1st policy period >= Mean loss in 2nd
Alternate Hypothesis: Mean loss of new drivers in 1st policy period < Mean loss in 2nd

#### 2. STATISTICALLY TESTING HYPOTHESIS (jumping to step 4)
The idea is to reject null hypothesis by proving that the difference observed in the population parameter 
and sample parameter is not just a matter of chance and but a systematic difference. This is often cited as  
testing if the difference is **statistically significant**  

**CHOICE OF SAMPLING DISTRIBUTION OR TEST STATISTIC AND**  
**SIGNIFICANCE LEVEL/CONFIDENCE FOR THE TEST AND**  
**CRITICAL VALUES AND REGION DETERMINATION** 
1. We use CLT , sample size and test metric to determine the choice of sampling  
  distribution and hence test statistic  
2. We also choose a confidence level / significance level, using which null hypothesis can be  
  confidently rejected. 
  *Signficance level* is a probability that null hypothesis is true and yet  
  as a matter of chance, we reject it based on sample observation. It represents what is called 
  Type 1 error (more on this later).We choose to minimize the probability of making Type1 error due  
  to chance.  
  The choice of confidence level/significance level varies with the 'research problem'  
  
3. Based on the hypothesis and confidence level, we choose the critical value, i.e the value used to  
   make the decision of accepting/rejecting null hypothesis such that even if we make the Type 1 error,  
   it is tolerable.  
   And then we compare the sample observation to this 'Critical Value' to determine the   
   decision of rejecting or not being able to reject Null Hypothesis.  
  
  Critical region : left tail,right tail, two tailed, formed by the critical value and tail/tails,  
  determined by sign of inequality of alternate hypothesis.  
  
- Example with test metric = mean, sample size >30.  
  Average waiting time in retail stores for customer is 2.6 mins.Based on a sample of 50 obs.,  
  it was found to be 2.9 mins with std. deviation of 0.5 mins in store X.  
  Do the customers in store X wait longer than that the retail store average.  
  
  H0 : mu <= 2.6  
  H1 : mu > 2.6   
  
  1. Sampling distribution of sample mean is 'normal' per CLT for large sample sizes ,so the test  
    statistic should be 'z',and mean of sampling distribution of means =  population mean, 
    std. error = sigma/sqrt(n).  
    But population std. deviation is not known, so we take distribution as t.But let us try  
    with normal first.  
    So, we have a nomal distriution with mean as population mean, and the observed sample mean  
    can be pictured on this distribution a certain distance away from population mean.  
  2.The test is whether this distance is significant, and signifiance brings into picture an  
    assumption of **significance/confidence** we are comfortable with. If we take it as 5%,  
    it means in 5/100 samples, we could get an observation that could lead us to reject a true  
    null hypothesis. 
  3. Using the probability of significance, we find the critical value pertaining to 5%.  
     First we see it is a right tailed test seeing the alternate hypothesis.  
     (Sign of alternate hypothesis determines the critical region, if the hypothesis  
      are stated correctly as defined above.)  
     
     
```{r}
# Convert the sampling distribution with mean 2.6 an sd 0.5 to standard normal
# Then convert sample observation to z stat. on this same scale
# i.e 
z = (2.9-2.6)/(0.5/sqrt(50))
z.crit = qnorm(p = 0.95,mean = 0,sd = 1,lower.tail = TRUE)
```

This is a right tailed test, critical region lies right to the critical value.  
z statistic lies lies in the critical region,i.e beyond the critical value,  
so we can reject the null hypothesis.

```{r}

# To picture ths values
r.norm <- sort(rnorm(n = 50))
f.norm <- dnorm(x = r.norm, mean = 0,sd = 1 )
plot(x = r.norm,y = f.norm,xlim = c(-4,4))
abline(v = c(z.crit,z), lty = c(2,1))
text(x = c(z.crit,z),y = c(0.005,0.005),labels = c("z.crit","z"))
```

The test statistic used should have been t. 

```{r}
# Convert the sampling distribution with mean 2.6 and sd 0.5 to t distribution
# Then convert sample observation to t stat. on this same scale
# i.e 
t = (2.9-2.6)/(0.5/sqrt(50))
t.crit = qt(p = 0.95,df = 49,lower.tail = TRUE) # df 49 as s was used
# To picture ths values
r.t <- sort(rt(n = 50,df = 49))
freq.t <- dt(x = r.t, df = 49 )
plot(x = r.t,y = freq.t,xlim = c(-4,4))
abline(v = c(t.crit,t), lty = c(2,1))
text(x = c(t.crit,t),y = c(0.05,0.05),labels = c("t.crit","t"))

```

### TYPES OF HYPOTHESIS TESTS AND TEST STATISTICS USED

### CONTINUOUS MEASURES (TBD?)
1. COMPARE SAMPLE MEAN TO A POPULATION MEAN 
  1.1 SAMPLE SIZE >30 & POP. STD. DEVIATION KNOWN : Z STAT.      
  1.2 LESS THAN 30 OR POP. STD. DEVIATION NOT KNOWN : T STAT.  
  
2. COMPARE SAMPLE PROPORTION TO A POPULATION PROPORTION  

3. COMPARE TWO POPULATION MEANS  
  PAIRED OBSERVATION (Z TEST ) VS SEPARATE SAMPLE TEST (INDEPENDENT T TEST), FIRST IS BETTER   
  EXPEERIMENT DESIGN  
  3.1 SAMPLE SIZE >30 AND POP. MEANS ARE KNOWN : Z TEST    
  3.2 LESS THAN 30 OR POP MEAN UNKNOWN : INDEPENDENT T TEST  
  
  In both the cases, the sampling distribution of difference of means is considered, the standard error of  
  sampling distribution is essentially sqrt. of sum of variances of sampling distribution of means of   
  individual populations.  
  sqrt(sigma1^2/n1 + sigma1^2/n2) .  
  The degree of freedom calculation varies on whether population std. deviation is known or not. 
  MORE ELABORATION NEEDED UNDER A SEPARATE PROJECT
  
  EXAMPLES:
  * A problem of interest could be to compare the sweetness on a scale for two cold drinks.If we use the  
    same set of sample, it is a paired observation test, depending on if the population std. deviations  
    are known, and sample sizes chosen, we make choise of sampling distribution and hence test stat.  
  * It is presumed that customer shopping for policies online gives a better overall shopping experience 
    with information submission time less than 5 mins, than those who take more than 5 mins.  
    So, submission time is an important metric for customer satisfaction score taken on a scale of 1:10.
    
    Sample of 15 customers each are taken from the two groups, one that took <5 mins and other that took 
    >5 mins, and scores are taken on scale of 1:10  
      
    |Group|N|Mean Score|SD|  
    |---|---|---|---|
    |<5mins|15|9.3|1.5|  
    |>5mins|15|7.9|1.9|  
    
    Hypothesis:  
    H0 : µ1 - µ2 = 0
    H1 : µ1 - µ2 > 0 
    
    Sampling Distribution/Test Statistic/Significance Level:  
    * The sampling distribution of difference of sample means will follow CLT. Here we don't know the  
      population sigmas, sample sizes are small as well.We don't know if the the underlying populations are normal, 
      to accomodate for small samples, and if their varinces are  equal or not.  
    * A simplifying assumption we take is that scores are normally distributed in two underlying populations,
      that compensates for small samples, now can say sampling distribution will be approx. t.
    * standard error and degrees of freedom of sampling distribution can be computed in two ways depending  
      on whether the variance of underlying population is same or different. If they are different, the  
      distribution can be approximated as t, but a possible better way is to use non-parametric methods.  
      If the variances are equal ( this can tested using F-test), then -
    

```{r}
# pooled sd of two samples
 sd.pop.pooled <- sqrt((1.5^2 * (15-1) + 1.9^2 * (15-1)) / (15-1 + 15 -1 ))
# used to find std. error
 std.error <- sqrt(sd.pop.pooled ^2 / 15 +  sd.pop.pooled ^2 / 15)
 df <- (15 - 1) + (15 - 1)
 # Take significance level as 5%
 # Under null hypothesis, the difference of samples means is ditributed with t, with mean as 0
 # and std. deviation as std.error
 # The t statistic of observed sample differences
 t <- (9.3 - 7.9) / std.error
 t.crit <-  qt(p = 0.95,df = df,lower.tail = TRUE)  
 print(c(t,t.crit))

```

DECISION : Reject Null Hypothesis

4. COMPLARE TWO POPULATION PROPORTIONS    
  
5. COMPARE VARIANCES OF TWO POPULATIONS : F TEST  

 We can draw different size samples from two populations assumed to be normally distributed. A sample drawn from
 a **normally distributed** population fulfills the property that  
 (n-1)s^2/sigma^2 ≈ X2, is a chisquare random variable with (n-1) df  
 
 For two populations, assumed to be normally distributed, if the population variance is same, then the ratio  
 of sample variance follows an F distribution  as,  
 
 s1^2 / s2^2 = (X1^2 . sigma1^2) / (n1-1)
 $\frac{s_1^2}{s_2^2} = \frac{\frac{\chi_1^2 . \sigma_1^2}{n_1-1}}{\frac{\chi_2^2 . \sigma_2^2}{n_2-1}}$  
 ![](http://latex.codecogs.com/gif.latex?%5Cfrac%7Bs_1%5E2%7D%7Bs_2%5E2%7D%20%3D%20%5Cfrac%7B%5Cfrac%7B%5Cchi_1%5E2%20.%20%5Csigma_1%5E2%7D%7Bn_1-1%7D%7D%7B%5Cfrac%7B%5Cchi_2%5E2%20.%20%5Csigma_2%5E2%7D%7Bn_2-1%7D%7D)  
  and can be seen to become the F statistic  
  ![](http://latex.codecogs.com/gif.latex?%5Cfrac%7Bs_1%5E2%7D%7Bs_2%5E2%7D%20%3D%20%5Cfrac%7B%5Cfrac%7B%5Cchi_1%5E2%20%7D%7Bn_1-1%7D%7D%7B%5Cfrac%7B%5Cchi_2%5E2%20%7D%7Bn_2-1%7D%7D)  
  
  Null Hypothesis is always, that population variances are equal and thus we say, the ratios of square sample  
  sds follow an F distribution with given dfs.
  Based on hypothesis formulation, we look to do a left, right or two tailed test with a chosen significance  
  level, understanding the under null hypothesis that F statistic is 1, we are looking if the distance calculated  
  from sample F statistic is significant from 1.  
  
  Example: Comparing the footfalls in two stores of a company to check that variance in footfall is different.  
  H0 : sigma1 = sigma2
  H1 : sigma1 # sigma2
  
 
```{r}
# For 7 days footfalls are recorded
s1 <- c(214,185,214,186,182,220,220)
s2 <-  c(171,185,236,175,227,198,172)
sd.s1<- sd(s1)
sd.s2 <-  sd(s2)
df1 <- 6
df2 <- 6
# Under null hypothesis, Sampling distribution or test statistic is F, df1 = df2 =6
# Two tailed test, critical points can be computes using signficance level of 5%
f.stat <- (sd.s1/sd.s2)^2
f.citical.right <- qf(p = 0.975,df1 = df1,df2 = df2)
f.citical.left <- qf(p = 0.025,df1 = df1,df2 = df2)

# For visualization
x <- sort(rf(n = 100,df1 = df1,df2 = df2))
f.freq <- df(x = x,df1 = df1, df2 =df2)
plot(x = x,y = f.freq,type = "p")
abline(v = c(f.citical.left,f.citical.right,f.stat),lty = c(2,2,1))

```

Decision : Cannot Reject Null Hypotheis

### CATEGORICAL MEASURES LIKE COUNTS AND FREQUENCIES

#### CHI-SQUARE GOODNESS OF FIT FOR COUNT DATA
We look at a categorical variable in a dat set, and summarize the counts across levels.  
These are observed counts.  
We may also have expected counts of this data from a population or could derive them based on  
null hypothesis. The questions is how well the observed data fits the expected.   
The number of categories k, each having a count that is considered a random variable, as the 
number of categories increse, the observed count becomes normal random variable. This is  
analogus to binomial distributon with n trials and r successes, with p as prob. of success  
approaching normal.  

If, Null Hypothesis is true,
For a large sample, the below statistic follows a chi-square distribution with df = k-1 assuming,  
as we are adding squares of normal random variables, that are independent i.e what goes in a cell  
is not dependent on what goes in other.
  
Ho : Expected and Observed Counts are equal  
H1 : They are not  

$ \sum_{i=1}^{n}{}\frac{(O_i - E_i)^2}{E_i}$  
![](http://latex.codecogs.com/gif.latex?%24%20%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%7B%7D%5Cfrac%7B%28O_i%20-%20E_i%29%5E2%7D%7BE_i%7D%24)  

A thumb of rule for large sample is that the expected value in each cell should be >=5.  
If it is not i.e samples is small, then cells could be combined to ensure validity of test.


EXAMPLE : auto policies sold follow a distributon amongst the vehicle types of 5 categories as below,
a sample has the following distribution.
H0: Observed equals Expected
H1 : Does not

```{r}
expected.prop <- c(0.3,0.24,0.3,0.11,0.05)
sample <- c(35,16,13,19,17)
expected.count <- expected.prop * sum(sample)
# expected count is >5 for each category, assumption of large sample is satisfied
chsq.stat <- sum((((sample - expected.count)^2) / expected.count))
chsq.stat
# Choice of significance level
qchisq(p = c(0.025,0.0975),df = 4)

```

DECISION : Reject the null hypothesis

#### CHI SQUARE TEST OF INDEPENDENCE (TBD)

### P VALUE, DECISION ERRORS, POWER OF A TEST







