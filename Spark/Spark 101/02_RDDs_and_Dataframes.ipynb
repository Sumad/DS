{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient Distributed Data Sets   and DataFrames\n",
    "### 1. What is RDD    \n",
    "** Primary abstraction of spark. Basically a dataset partitioned across cluster of machines. Defined as a fault tolerant collection of elements that can be operated on in parallel, they are also immutable **  \n",
    "### 2. How can they be created    \n",
    "** Three methods ** -   \n",
    "1. parallelizing data in spark, meaning distributing it across machines. Parallelizing is an operation that returns a pointer.  \n",
    "2. Reading from any storage supported by hadoop  \n",
    "  - Cassandra  \n",
    "  - HBase  \n",
    "  - HDFS  \n",
    "  - Amazon S3 etc  \n",
    "Multiple types of files can be read -\n",
    "  - text, sequence, hadoop input format   \n",
    "Reading from any of these sources creates an RDD and a pointer is returned.  \n",
    "3. From other RDDs,  when a transformation operation is performed  \n",
    "\n",
    "#### 3. What happens when an RDD is created   \n",
    "A DAG is created when an RDD is created\n",
    "#### 4. What Operations can be peformed on them    \n",
    "Transformations - These update the DAG  , and return a pointer to the RDD to be created, but not the value  \n",
    "Action - The DAG is evaluated when an action is called and return a value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Scala : Creating and working with RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The location where spark is installed is noted by environment variable - $SPARK_HOME. Launch spark sheel from SPARK_HOME/bin  \n",
    ".bin/spark_shell  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create some data \n",
    "val data = 1 to 10000     # val keyword is used to declare a value carrying object\n",
    "# praellize the data and create an RDD    \n",
    "val distData = sc.parallelize(data)  # sc is available in the environment\n",
    "# Perform a transformation \n",
    "distData.filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Another way is load a file \n",
    "val data = sc.textFile(\"file.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading from hdfs \n",
    "val data = sc.textFile(\"hdfs://lines.txt\") \n",
    "# Apply transformation \n",
    "val llength = data.map(line => line.length)\n",
    "# Invoke action \n",
    "val totallth = llength.reduce((a,b) => a+b)  # function arguments with a binary operation is stated as (a,b) \n",
    "followed by definiton\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Count example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val words = data.flatMap(line => line.split(\" \"))\n",
    ".map(word => (word,1))\n",
    ".reduceByKey((a,b) => a+ b)\n",
    "\n",
    "words.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Direct Acyclic Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ater transformations have been applied on an RDD, the DAG can be viewed. An example of available method in Scala is \n",
    "# toDebugString:\n",
    "val rdd1 = readme.flatMap(line => line.split(\" \")) \n",
    "val rdd2 = rdd1.map(wrd => (wrd, 1))\n",
    "val rdd3 = rdd2.reduceByKey((a,b) => a+b)\n",
    "rdd3.toDebugString\n",
    "\n",
    "# In Python DAG can be seen as\n",
    "rdd1 = logFile.filter(lambda line : 'INFO' in line )\n",
    "rdd2 = rdd1.flatMap(lambda line : line.split(' ') )\n",
    "rdd3 = rdd2.map(lambda wrd : (wrd, 1))\n",
    "rdd4 = rdd3.reduceByKey(sum)\n",
    "rdd3.toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Example:\n",
    "# (2) ShuffledRDD[30] at reduceByKey at <console>:29 []\n",
    "# +-(2) MapPartitionsRDD[29] at map at <console>:27 []\n",
    "#    |  MapPartitionsRDD[28] at flatMap at <console>:25 []\n",
    "#    |  MapPartitionsRDD[2] at textFile at <console>:23 []\n",
    "#   |  /resources/jupyter/labs/BD0211EN/LabData/README.md HadoopRDD[1] at textFile at <console>:23 []\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DAG when read from bottom to top, above shows transformations starting from when the text file is read, followed by two map, and a reduceby Key transformation. \n",
    "** Falult tolerance is enable on node failure,  by the behavior of copying over this DAG from another node and executing the DAG till the point of failure **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step by Step execution of DAG  with an example of log file analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. When the file is first read it is partitioned across nodes in the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Count occurences of 'insecure' and 'authentication' in a log file with warning messages\n",
    "val logFile = sc.textFile(\"notebook.log\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. The driver send the below transformations and actions to be performed to each node. The exectors on each node *read the data from the node* and perform the tasks in parallel. After performing the tasks, the results are returned to the driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val warn = logFile.filter(line => line.contains(\"WARNING\") )\n",
    "val tokens = warn.flatMap(line => line.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Caching - another transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Caching\n",
    "tokens.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Action 1 - After completion of first action, the RDD tokens is cached "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Action 1\n",
    "tokens.filter(word => word == \"insecure\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Action 2 - this action uses the cached RDD now, and is much faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Action 2\n",
    "tokens.filter(word => word == \"encryption\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. RDD persistence "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark allows RDD partitions and partitions created by transformations on the nodes to be stored in memory, thereby making any \n",
    "later computations to be fast (almost 10X). An important feature that must be leveraged for an iterative work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two methods - \n",
    "1. perist()  \n",
    "2. cache() -  which is persist with MEMORY_ONLY storage   \n",
    "\n",
    "STORAGE LEVEL--------------  Meaning\n",
    "MEMORY_ONLY       ---       Persist RDD in memory only  \n",
    "MEMORY_AND_DISK   ---       Spill over to disk only if necessary, and then read fro disk when an action is required  \n",
    "MEMORY_ONLY_SER   ---       Memory only, but save as serialized java object, which take less space, but need derialization and \n",
    "                            increase CPU time  \n",
    "MEMORY_AND_DISK_SER ---      Serialized   \n",
    "DISK_ONLY           ---      Disk only option  \n",
    "MEMORY_ONLY_2,DISK_ONLY_2,      ---     Make a copy on two nodes, persist in memory or disk resp.      \n",
    "OFF HEAP            ---    Option of executors using shared memory   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If RDD is small and likely to fit in memory, use cache()  \n",
    "If not, try to use serialization option, but with a fast serializer   \n",
    "Try not to spill to disk, unless expensive computations are being used   \n",
    "Tachyon is a good option, when your environment has multiple application and high memory requirements.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Shared Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two types of shared variables are available in spark  \n",
    "1. Broadcast variables    \n",
    "usually separate set of variables are used in each node for performing computations. Broadcast variables are useful in passing  \n",
    "something 'read only' information to worker nodes, these variable are immutable.\n",
    "2. Accumulators  \n",
    "  - Are variables that are passed to worker nodes, and can be added to by worker nodes.  \n",
    "  - Only driver can read the accumulator values.  \n",
    "  - These are used to implement counters and sums.  \n",
    "  - Spark natively supports numeric types, but other types can be defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creat a broadcast variable\n",
    "val bcVar = sc.broadcast(Array(1,2,3,4))\n",
    "# Access value\n",
    "bcVar.value\n",
    "\n",
    "## Python\n",
    "broadcastVar = sc.broadcast([1,2,3])\n",
    "broadcastVar.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an Accumulator var\n",
    "val acc = sc.accumulator(0)\n",
    "acc.value\n",
    "\n",
    "## Python\n",
    "accum = sc.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an RDD and try performing associative operation to add to accumulator\n",
    "rdd = sc.parallelize(Array(1,2,3,4))\n",
    "rdd.foreach(x => acc+=x)    # accumulator variable was accesable indide function foreach\n",
    "# Check acc value\n",
    "acc.value\n",
    "\n",
    "## Python \n",
    "rdd = sc.parallelize([1,2,3,4])\n",
    "def f(x):                   # to access variable accum, have to define it as global inside the function\n",
    "    global accum\n",
    "    accum += x\n",
    "\n",
    "rdd.foreach(f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Key Values Pairs and Programming with them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ** There are special operations available for RDDs with key value pairs . ** \n",
    "- **Common operations are grouping and aggregating by keys, like reduceByKey**   \n",
    "- **Require using Tuple2 objects which can be created by notation (a,b) in scala, but require importing spark context library as \n",
    "  import org.apache.spark.SparkContext._  **    \n",
    "- ** Pair RDD function contain key value operations like reduceByKey((a,b) => a+b) **    \n",
    "- ** Custom objects as key-value pairs, require a custom methods (equals() method with a matching hashCode() method?) **  \n",
    " Key Value pairs in Scala  \n",
    "val pair = ('a', 'b'), Element accessed like a._1 and a._2    \n",
    " In Python     \n",
    "pair = ('a','b') , Eg: accessed using a[0], a[1]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val rdd1 = readme.flatMap(line => line.split(\" \"))   \n",
    "val rdd2 = rdd1.map(wrd => (wrd, 1))  # created a RDD of key value pairs   \n",
    "val rdd3 = rdd2.reduceByKey(_+_) # enable using Paired RDD operation, _+_ is a shorthand of the sum operation to be done on\n",
    "values  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################# Example with NYC dataset #######################\n",
    "val taxi = sc.textFile(\"/resources/jupyter/labs/BD0211EN/LabData/nyctaxi.csv\")\n",
    "val taxiParse = taxi.map(line=>line.split(\",\"))\n",
    "val taxiMedKey = taxiParse.map(vals=>(vals(7), 1))  # Create a paired RDD to later group on medallion and count cars by \n",
    "# medallion number\n",
    "val taxiMedCounts = taxiMedKey.reduceByKey((v1,v2)=>v1+v2)\n",
    "\n",
    "# Swap the values in tuples to later present the results in sorted\n",
    "for (pair <-taxiMedCounts.map(_.swap).top(10)) println(\"Taxi Medallion %s had %s Trips\".format(pair._2, pair._1))\n",
    "\n",
    "# Note ths shorthand of using - (.swap) instead of (  x => x.swap())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Joining RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read , filter one file , make a paired RDD\n",
    "## Scala\n",
    "val readmeCount = readmeFile.filter(line => line.contains(\"Spark\")).\n",
    "flatMap(line => line.split(\" \")).\n",
    "map(wrd => (wrd,1)).\n",
    "reduceByKey((a,b) => a+b)\n",
    "readmeCount.count()\n",
    "\n",
    "## Python\n",
    "readmeCount = readmeFile.                    \\\n",
    "    flatMap(lambda line: line.split(\"   \")).   \\\n",
    "    map(lambda word: (word, 1)).             \\\n",
    "    reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Repeat for second RDD\n",
    "## Scala\n",
    "val pomCount = pom.filter(line => line.contains(\"Spark\")).\n",
    "flatMap(line => line.split(\" \")).\n",
    "map(wrd => (wrd,1)).\n",
    "reduceByKey((a,b) => a+b)\n",
    "pomCount.count()\n",
    "\n",
    "## Python \n",
    "pomCount = pomFile.                          \\\n",
    "    flatMap(lambda line: line.split(\"   \")).   \\\n",
    "    map(lambda word: (word, 1)).            \\\n",
    "    reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do an inner join \n",
    "## Scala\n",
    "val joined = readmeCount.join(pomCount)\n",
    "\n",
    "## Python\n",
    "joined = readmeCount.join(pomCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Count 'Spark' keyword in both and print\n",
    "## Scala\n",
    "joined.map(a => (a._1,(a._2)._1 + (a._2)._2) ).\n",
    "collect.foreach(println)\n",
    "\n",
    "## Python\n",
    "joinedSum = joined.map(lambda k: (k[0], (k[1][0]+k[1][1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame using Python API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Explore : **    \n",
    "    1. How to create a Spark DataFrame  \n",
    "    2. Perform group by and aggregation operations   \n",
    "    3. Running SQL queries on spark data frame \n",
    "\n",
    "In python Pandas library provides data frame structure, and ways to create and manipulte data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating Spark Data Frame required creating a SQLcontect from Spark Context\n",
    "sqlcontext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a spark dataframe using a pandas data frame, and methods available in sqlcontext\n",
    "sdf = sqlcontext.CreateDataFrame(mtcars)\n",
    "# Describe the frame\n",
    "sdf.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Explore sample rows -  show() method\n",
    "sdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Selecting columns - select method\n",
    "sdf.select('mpg').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filtering using filter method \n",
    "sdf.filter(sdf['mpg'] < 10). showd(5)        # Indexing usng [] works as for pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating a new column - using withColumn()\n",
    "sdf2 = sdf.withColumn('wtTon', sdf['wt'] * 0.45) # returns a new spark dataframe, original is not modified as in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Groupig and Aggregation\n",
    "sdf3 = sdf.groupby(['cyl'])\\\n",
    ".agg({\"wt\": \"AVG\",\n",
    "     \"mpg\" : 'SUM'})\\\n",
    ".show(5)             # Dictionary inside agg method to specify aggregation on column, and method\n",
    "sdf3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sorting \n",
    "sdf3.sort('count(wt)', ascending =  False).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running SQL Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1. Register data frame as a table using registerTempTable method\n",
    "cars = sdf.registerTempTable(\"cars\")\n",
    "# 2. # SQL statements can be run by using the sql method\n",
    "highgearcars = sqlcontext.sql(\"SELECT gear FROM cars WHERE cyl >= 4 AND cyl <= 9\")\n",
    "highgearcars.show(6) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
