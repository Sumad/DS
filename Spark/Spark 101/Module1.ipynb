{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Spark, how is it different from Map reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gorwing Data : cluster computing required for manipulating huge size of data  \n",
    "A. mapreduce vs spark  \n",
    "1. Speed  \n",
    "- mapreduce used n/w to shuffle, writing to disk to provide failure resistant experience of nodes  \n",
    "- spark does in memory computation, provides failure resistant exp. by saving the ops.   \n",
    "  to be performed on data, and reapplying them on reovery from the point of failure. So, less n/w operation involvment.  \n",
    "  Even faster writing to disk. Use of functional programming constructs    \n",
    "- So, faster than map reduce, diff. becomes wider as scale increases  \n",
    "2. Generality  \n",
    "- useable for multiple use cases    \n",
    "- iterative tasks like ML become much easier to work with   \n",
    "3. Ease of Use   \n",
    "- Runs on hadoop cluster with scheduler like Yarn or Apache Mesos or even standalone cluster  \n",
    "- APIs for scala, R, Python, Java  \n",
    "- libraries for SQL, streaming, graph processing. , ML  \n",
    "- Interactive programming   \n",
    "\n",
    "So, overall : spark wins in memory distributed computing, with low latency; APIs and libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Unified Stack, RDD and key operation types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Unified Stack :   \n",
    "\n",
    "-- Spark SQL, Spark Straming, MLlib, Graph X  --- libraries  \n",
    "-- Spark Core --  \n",
    "-- Scheduler with Yarn, Mesos or in built scheduler  \n",
    "\n",
    "Spark Core:  \n",
    "1.  RDD : Resilient Data Set is the primary data abrstraction of spark  \n",
    "- Distributed collection of elements, parallelized across cluster    \n",
    "- **Types of Operations on RDD **  \n",
    "A. **Transformations **\n",
    "- Like a sequence of operations applied on data , just creates a Direct Acyclic Graph of    \n",
    "  operations, no evaluation, nothing returned \n",
    "- As operations are added, DAG is updated  \n",
    "B. **Actions **\n",
    "- Prompt evaluation aka Lazy Evalution, DAG is evaluated when action is called  \n",
    "- DAG updation, Lazy evaluation let Spark be resilient to failures. On failure, the DAG is   \n",
    "  re-evaluated  from where it was  \n",
    "- Cach memory is available in spark to do processing in memory, if memory is not suofficient,   \n",
    "  disk memory is used   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scala Overview , Starting spark shell for Scala and Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spark is written in Scala  \n",
    "- Everything in scala is object : basic data types like numbers, functions    \n",
    "- Function are objects, so they can be passed as args to other fxs, returned from a fx  ,\n",
    "  stored in vars.   \n",
    "- Function syntax : def funcname ([list of args]) : [return type]\n",
    "- Starting Spark shell from Scala and Python  \n",
    "**1. Scala **  \n",
    " .bin/spark-shell  \n",
    " val textfile = sc.textFile('fname')  \n",
    "**2. Python ** \n",
    " .bin/pyspark  \n",
    " textfile = sc.textFile('fname')  \n",
    "here sc is Spark COntext which is available as an object \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Methods for RDD in Python API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**filter** - Return a new RDD after applying a specified filtering function on each element of an RDD.  \n",
    "Define a filtering function and apply it by passing in filter() method of RDD\n",
    "Example of a sequence of transformation and action:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lt = sc.parallelize([1,2,3,4,5]) # parallelize method creates an RDD from a non-distributed object\n",
    "lt_tfr = lt.filter(lambda x : x if(lt = sc.parallelize([1,2,3,4,5]) # parallelize method creates an RDD from a non-distributed object\n",
    "lt_tfr = lt.filter(lambda x : x % 2 == 0) # transformation\n",
    "lt_action = lt_tfr.collect() # action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**map(f, preservesPartitioning=False) **   \n",
    "Return a new RDD by applying a function to each element of this RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**reduce(f)**  \n",
    "Reduces the elements of this RDD using the specified **commutative and associative binary operator**. Currently reduces partitions locally  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "sc.parallelize([1, 2, 3, 4, 5]).reduce(lambda a,b : max(a,b) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groupByKey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReduceByKey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Transformations and Action operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test spark version  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "readme = sc.textFile(\"README.md\") # usually will be Reading a distributed file on a cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check first line, Count number of lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "readme.first()\n",
    "readme.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count no. of line that contain word 'Spark'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count maximum no. of words in a line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
