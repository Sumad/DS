{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Summary -**      \n",
    "** 1. Spark Context  - in spark shell, how to invoke and use in programming an app.**  \n",
    "** 2. Using Spark with Scala - Essentials  **  \n",
    "** 3. using Spark with Python - Essentials **    \n",
    "** 4. Initializing Spark - Scala and Python **   \n",
    "** 5. Passing functions to spark **   \n",
    "** 6. Create and run standalone spark application **  \n",
    "** 7. Run standaline spark application  **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Spark Context   \n",
    "  - Entry point to spark when working with spark shell, need to import the following libraries in Scala when working in an app. \n",
    "    - import org.apache.spark.SparkContext  \n",
    "    - import org.apache.spark.SparkConf \n",
    "    - import org.apache.spark.SparkContext._   # useful for implicit conversions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 . Spark with Scala  \n",
    "- Need to have spark and scala version that are compatible  \n",
    "    - To write a spark applicationm, need to add a maven dependency. Spark is available through maven central at:\n",
    "        - groupId = org.apache.spark  \n",
    "        - artifactId - spark-core_2.1.0  \n",
    "        - version - <> \n",
    "    - If operating on a hdoop cluster need to add a dependency on on hadoop client for the version of hdfs    \n",
    "        - groupId = org.apchae.hadoop\n",
    "        - artifactId = hadoop-client   \n",
    "        - version = <> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 . Spark with Python   \n",
    "  - Need to have python version compatible with spark version   \n",
    "  - Import classes as    \n",
    "          from pyspark import SparkContext, SparkConf\n",
    "  - To run spark appls. in python, submit the apps. using .bin/spark-submit script located in spark directory  , which\n",
    "          - LoadsSpark's scala/java libraries   \n",
    "          - allows submitting apps. to cluster   \n",
    "  - To use Hadoop cluster, need to used a pyspark build linking your version of hdfs        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Initializing Spark   \n",
    "After the dependencies are taken care of, one needs to build a SparkConf object  containing information about one's app.  \n",
    "### Scala   \n",
    "val conf = SparkConf().setAppName(appname).setMaster(master)   \n",
    "  - app name is a name to give to app.  \n",
    "  - master is the url of yarn, sark, mesos cluster or 'local' to run in local mode. local[16] will allocate 16 cores, in production, do not hardocode master, rather pass it in spark-submit  \n",
    "  - then create a SparkContext object  using the conf just created\n",
    "  - new SparkContext(conf)\n",
    "  \n",
    "### Python   \n",
    "-  Everything is same execp the syntax of contruction of SparkCOntext \n",
    "  - sc = SparkContext (conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Passing Functions to spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a. Using anaonymous functions \n",
    "  - lambda functions in python  \n",
    "  - Scala:\n",
    "    - Note that a functions in general is defined as -  def func1 (rdd: RDD[String]) : RDD[String] { }  \n",
    "    - Anonyous fxs. are as an eg.  (x: Int) => x +1    \n",
    "\n",
    "b. # Scala   \n",
    "   Passing a global object with a function  \n",
    "    object Functions{\n",
    "        def func1 (rdd: RDD[String]) : RDD[String {}\n",
    "    }\n",
    "   rdd.map(Functions.func1)      \n",
    "\n",
    "c.  Passing by reference   \n",
    "    Instead of passing a big object, passing a reference is memory efficient   \n",
    "    val field = \"ArbitBig data\"   \n",
    "    def func1 (rdd: RDD[String]) : RDD[String {rdd.map ( x => x + field)}  ## Not good\n",
    "      \n",
    "    def func1 (rdd: RDD[String]) : RDD[String {\n",
    "        field_ = this.field  # create a reference/not a copy \n",
    "        rdd.map ( x => x + field_)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Creating a spark application using the steps \n",
    "  - Create appropriate dependencies\n",
    "  - Import necessary classes  \n",
    "  - Create SparkConf and then SparkContext   \n",
    "  - Create RDD, apply transformations and actions   \n",
    "  - Use RDD persistence  to improve performance   \n",
    "  - Use broadcasr and accumulator vars.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Scala app to count no. of lines of a's and b's in a log file  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.SparkContext\n",
    "import org.apache.SparkContext_\n",
    "import org.apache.SparkConf\n",
    "\n",
    "object ScalaApp{\n",
    "    def main(args:Array[String]){\n",
    "        # Create Spark Conf and Spark Context\n",
    "        val Conf = new SparkConf().setAppName(\"MyScalaApp\")\n",
    "        val sc = new SparkContext(Conf)\n",
    "\n",
    "        # Transformations, Actions, Persistence\n",
    "        val data = sc.textFile(\"log file\").cache()\n",
    "        rdda = data.filter(line => line.contains(\"a\").count()\n",
    "        rddb = data.filter(line => line.contains(\"b\").count() \n",
    "        println(\"Lines with a are {0} and b are {1}\".format(rdda, rddb))\n",
    "                            }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    ### Python App, written in a .py file\n",
    "    from pyspark import SparkContext, SparkConf\n",
    "    if __name__ == \"__main__\":\n",
    "\n",
    "        conf = SparkConf().setAppName(\"MyScalaApp\")\n",
    "        sc = SparkContext(Conf)\n",
    "        data = sc.textFile(\"log file\").cache()\n",
    "        rdda = data.filter(lambda line : 'a' in line).count()\n",
    "        rddb = data.filter(lambda line : 'b' in line).count()\n",
    "        print(\"Lines with a are {0} and b are {1}\".format(rdda, rddb))\n",
    "        sc.stop() # stop the spark context\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.Running standalone spark app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define the dependencies using any system builds (Ant, sbt, Maven). Varies for scala and python. Example files - \n",
    " scala - scala.sbt  \n",
    " python - py--files argument can be used      \n",
    "2. Create a typical directory structure   Eg. for scala  \n",
    "   ./simple.sbt  \n",
    "   ./ src  \n",
    "   ./src/main  \n",
    "    ./src/main/scala\n",
    "    ./src/main/scala.simpleApp.scala\n",
    "\n",
    "3. Create a JAR package containing application code. in   Python, create a set of .py files\n",
    "4. Use spark submit to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Spark Submit i.e submitting app. to cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Typical params are explained bellow \n",
    "./bin/spark-submit \\\n",
    "-- class <>            # main entry point to\n",
    "-- master <>           # master URL, recommended to be provided here \n",
    "-- deploy-mode <>      # do you want to deploy driver on a worker node or locally as an external client (default option)\n",
    "-- conf <key>-<value>  # config options, set for many spark properties like nodes, accumulator,s memory allocation et al\n",
    "... #other options \n",
    "<application-jar> \n",
    "[application arguments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark-submit --help  # see more parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps 7 and 8 are detailed in Ex 3 document"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
