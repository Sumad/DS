{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Spark, how is it different from Map reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gorwing Data : cluster computing required for manipulating huge size of data  \n",
    "A. mapreduce vs spark  \n",
    "1. Speed  \n",
    "- mapreduce used n/w to shuffle, writing to disk to provide failure resistant experience of nodes  \n",
    "- spark does in memory computation, provides failure resistant exp. by saving the ops.   \n",
    "  to be performed on data, and reapplying them on reovery from the point of failure. So, less n/w operation involvment.  \n",
    "  Even faster writing to disk. Use of functional programming constructs    \n",
    "- So, faster than map reduce, diff. becomes wider as scale increases  \n",
    "2. Generality  \n",
    "- useable for multiple use cases    \n",
    "- iterative tasks like ML become much easier to work with   \n",
    "3. Ease of Use   \n",
    "- Runs on hadoop cluster with scheduler like Yarn or Apache Mesos or even standalone cluster  \n",
    "- APIs for scala, R, Python, Java  \n",
    "- libraries for SQL, streaming, graph processing. , ML  \n",
    "- Interactive programming   \n",
    "\n",
    "So, overall : spark wins in memory distributed computing, with low latency; APIs and libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Unified Stack, RDD and key operation types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Unified Stack :   \n",
    "\n",
    "-- Spark SQL, Spark Straming, MLlib, Graph X  --- libraries  \n",
    "-- Spark Core --  \n",
    "-- Scheduler with Yarn, Mesos or in built scheduler  \n",
    "\n",
    "Spark Core:  \n",
    "1.  RDD : Resilient Data Set is the primary data abrstraction of spark  \n",
    "- Distributed collection of elements, parallelized across cluster    \n",
    "- **Types of Operations on RDD **  \n",
    "A. **Transformations **\n",
    "- Like a sequence of operations applied on data , just creates a Direct Acyclic Graph of    \n",
    "  operations, no evaluation, nothing returned \n",
    "- As operations are added, DAG is updated  \n",
    "B. **Actions **\n",
    "- Prompt evaluation aka Lazy Evalution, DAG is evaluated when action is called  \n",
    "- DAG updation, Lazy evaluation let Spark be resilient to failures. On failure, the DAG is   \n",
    "  re-evaluated  from where it was  \n",
    "- Cach memory is available in spark to do processing in memory, if memory is not suofficient,   \n",
    "  disk memory is used   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scala Overview , Starting spark shell for Scala and Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spark is written in Scala  \n",
    "- Everything in scala is object : basic data types like numbers, functions    \n",
    "- Function are objects, so they can be passed as args to other fxs, returned from a fx  ,\n",
    "  stored in vars.   \n",
    "- Function syntax : def funcname ([list of args]) : [return type]\n",
    "- Starting Spark shell from Scala and Python  \n",
    "**1. Scala **  \n",
    " .bin/spark-shell  \n",
    " val textfile = sc.textFile('fname')  \n",
    "**2. Python ** \n",
    " .bin/pyspark  \n",
    " textfile = sc.textFile('fname')  \n",
    "here sc is Spark COntext which is available as an object \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Methods for RDD in Python API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transformations : filter, map , reduce  , reduceByKey, groupByKey  \n",
    "Actions : collect, count, take, foreach(func)  **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**filter** - Return a new RDD after applying a specified filtering function on each element of an RDD.  \n",
    "Define a filtering function and apply it by passing in filter() method of RDD\n",
    "Example of a sequence of transformation and action:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lt = sc.parallelize([1,2,3,4,5]) # parallelize method creates an RDD from a non-distributed object\n",
    "lt_tfr = lt.filter(lambda x : x if(lt = sc.parallelize([1,2,3,4,5]) # parallelize method creates an RDD from a non-distributed object\n",
    "lt_tfr = lt.filter(lambda x : x % 2 == 0) # transformation\n",
    "lt_action = lt_tfr.collect() # action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**map(f, preservesPartitioning=False) **   \n",
    "Return a new RDD by applying a function to each element of this RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**reduce(f)**  \n",
    "Reduces the elements of this RDD using the specified **commutative and associative binary operator**. Currently reduces partitions locally  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from operator import add\n",
    "sc.parallelize([1, 2, 3, 4, 5]).reduce(lambda a,b : max(a,b) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**flatMap(f, preservesPartitioning=False)**  \n",
    "Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groupByKey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**reduceByKey(func, numPartitions=None, partitionFunc=<function portable_hash at 0x7fc35dbc8e60>)**  \n",
    "Merge the values for each key using an associative and commutative reduce function.  \n",
    "\n",
    "This will also perform the merging locally on each mapper before sending results to a reducer, similarly to a “combiner” in MapReduce.  \n",
    "\n",
    "Output will be partitioned with numPartitions partitions, or the default parallelism level if numPartitions is not specified. Default partitioner is hash-partition.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** sortByKey(ascending=True, numPartitions=None, keyfunc=<function <lambda> at 0x7fc35dbcf848>)**  \n",
    "Sorts this RDD, which is assumed to consist of (key, value) pairs.   \n",
    "keyfunc can be used to specify the function used to sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**join(other, numPartitions=None)**  \n",
    "Return an RDD containing all pairs of elements with matching keys in self and other.\n",
    "\n",
    "Each pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in self and (k, v2) is in other.\n",
    "\n",
    "Performs a hash join across the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Examples with Python and Scala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Transformations and Action operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test spark version  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python \n",
    "readme = sc.textFile(\"README.md\") # usually will be Reading a distributed file on a cluster\n",
    "# Scala\n",
    "val readme = sc.textFile(\"README.md\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check first line, Count number of lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python\n",
    "readme.first()\n",
    "readme.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count no. of lines that contain word 'Spark'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter operation follwed by action count\n",
    "## Python \n",
    "readme.filter(lambda line : 'Spark' in line).count()\n",
    "## Scala\n",
    "readme.filter(line => line.contains(\"Spark\")).count()   # single quotes don't work in scala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice that instead of a lambda function, in scala an anonymous function is defined using the syntax   \n",
    "arg => function specification  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count maximum no. of words in a line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation and Actions can be seen as functions oriented to performing specific types of operations,  and taking arguments as functions defined to perform those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Break each line into words, count the words (transformations) and find max \n",
    "## Python\n",
    "readme.map(lambda line : len(line.split())).reduce(lambda a,b: max(a,b) )\n",
    "## Scala\n",
    "import java.lang.Math\n",
    "readme.map(line => line.split(\" \")).\n",
    "map(line => line.size).\n",
    "reduce((a,b) => Math.max(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# same as above, but not using anaonymous function in Python\n",
    "def max(a, b):\n",
    " if a > b:\n",
    "    return a\n",
    " else:\n",
    "    return b\n",
    "readme.map(lambda line : len(line.split())).reduce(max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count of Words / Freq. distribution of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# On RDD, use transformation - \n",
    "# a flat map to create a list of all words in lines, use map to create a tuple with count 1, then use reduceByKey to add counts \n",
    "# Action - possibly collect, if collect does not bring a large amount of data into driver node\n",
    "counts = readme.flatMap(lambda line: line.split()).map(lambda x: (x,1)).reduceByKey(add).collect()\n",
    "\n",
    "# Scala\n",
    "readme.flatMap(line => line.split(\" \")).\n",
    "map(wrd => (wrd,1)).reduceByKey((a,b) => a+b).\n",
    "take(5) # first five pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word with max counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python \n",
    "rdd1 = readme.flatMap(lambda x: x.split()).map(lambda x: (x,1)).reduceByKey(add)\n",
    "rdd1.reduce(lambda a,b : a if(a[1] > b[1]) else b)\n",
    "\n",
    "# Scala\n",
    "readme.flatMap(line => line.split(\" \")).\n",
    "map(wrd => (wrd,1)).reduceByKey((a,b) => a+b).\n",
    "reduce((a,b) => if(a._2 > b._2) a else b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Spark Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When operating on a small RDD or when needing to do repeated operations on a RDD, it might be speedy to cache the RDD (still in \n",
    "a distributed way), i.e all nodes save the partitioned data in memory.   \n",
    "A typical usage scenario will be doing an interative model training on the same data perhaps.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_spark = readme.filter(lambda line : 'Spark' in line)\n",
    "def count ():\n",
    "    l_spark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Timer helps time an operation by running multiple iterations\n",
    "from timeit import Timer\n",
    "t = Timer(count())\n",
    "t.timeit(number = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_spark.cache()\n",
    "t.timeit(number = 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
