{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resilient Distributed Data Sets  \n",
    "#### 1. What is RDD    \n",
    "** Primary abstraction of spark. Basically a dataset partitioned across cluster of machines. Defined as a fault tolerant collection of elements that can be operated on in parallel, they are also immutable **  \n",
    "#### 2. How can they be created    \n",
    "** Three methods ** -   \n",
    "1. parallelizing data in spark, meaning distributing it across machines. Parallelizing is an operation that returns a pointer.  \n",
    "2. Reading from any storage supported by hadoop  \n",
    "  - Cassandra  \n",
    "  - HBase  \n",
    "  - HDFS  \n",
    "  - Amazon S3 etc  \n",
    "Multiple types of files can be read -\n",
    "  - text, sequence, hadoop input format   \n",
    "Reading from any of these sources creates an RDD and a pointer is returned.  \n",
    "3. From other RDDs,  when a transformation operation is performed  \n",
    "\n",
    "#### 3. What happens when an RDD is created   \n",
    "A DAG is created when an RDD is created\n",
    "#### 4. What Operations can be peformed on them    \n",
    "Transformations - These update the DAG  , and return a pointer to the RDD to be created, but not the value\n",
    "Action - The DAG is evaluated when an action is called and return a value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scala : Creating and working with RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The location where spark is installed is noted by environment variable - $SPARK_HOME. Launch spark sheel from SPARK_HOME/bin  \n",
    ".bin/spark_shell  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create some data \n",
    "val data = 1 to 10000  \n",
    "# praellize the data and create an RDD    \n",
    "val distData = sc.parallelize(data)  # sc is available in the environment\n",
    "# Perform a transformation \n",
    "distData.filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Another way is load a file \n",
    "val data = sc.textFile(\"file.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading from hdfs \n",
    "val data = sc.textFile(\"hdfs://lines.txt\") \n",
    "# Apply transformation \n",
    "val llength = data.map(line => line.length)\n",
    "# Invoke action \n",
    "val totallth = llength.reduce((a,b) => a+b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Count example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val words = data.flatMap(line => line.split(\" \"))\n",
    ".map(word => (word,1))\n",
    ".reduceByKey((a,b) => a+ b)\n",
    "\n",
    "words.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
