{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Spark Libraries  \n",
    "Support differen uses cases  \n",
    "\n",
    "Spark SQL    \n",
    "Spark Streaming  \n",
    "MLlib   \n",
    "GraphX  \n",
    "-----------------\n",
    "Spark Core\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key features -  \n",
    "1. Extension of core API   \n",
    "2. Any improvement to core are passed to these libs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Allows SQL queruing by uses of \n",
    "  1. SQL  \n",
    "  2. HiveQL  \n",
    "  3. Scala   \n",
    "- Use a news type of RDD - Schema RDD, to be thought of like a relational DB table.   \n",
    "- Consists of row objects, and a schema that describes the data type in columns  \n",
    "- How is SchemaRDD created   \n",
    "  1. From an RDD  \n",
    "  2. From a parquet file\n",
    "  3. or a json dataset  \n",
    "  4. or using HiveQL against Apache Hive   \n",
    "  \n",
    "- SparkSQL application can be written in Python, Scala, Java  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating SparkSQL context object using sparkcontext object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Scala\n",
    "val sc: SparkContext # existing \n",
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)    \n",
    "\n",
    "##### Python\n",
    "from pyspark.sql import SQLContext   \n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to create SchemaRDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scala to create a schema RDD import Schema RDD , not required in python\n",
    "import sqlContext.SchemaRDD  \n",
    "\n",
    "Two ways to create the schema   \n",
    "1. Infer the schema   \n",
    "2. Use Programatic interface : when schema is not pre-known, and you expect to create it at run time  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SchemaRDD - Inferring schema using reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## SCALA\n",
    "#1. case class used to define a schema, arguments of case class become column names\n",
    " case class Person(id : String, age : Int)\n",
    "#2. Create an RDD of Person object  \n",
    "val people = sc.textFile(\"<file path.txt>\").\n",
    "map(_.split(\",\")).                           # splitting to separate age and column\n",
    "map(p => Person(p(0), p(1).trim.toInt))   # transforming each row to a person object\n",
    "\n",
    "#3. Register as temp table\n",
    "people.registerTempTable(\"people\")\n",
    "\n",
    "#4. Run SQL statements \n",
    "val teens = sqlContext.sql('select name from people where age > 13')\n",
    "\n",
    "# teens is a schemaRDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When you cannot define scheme ahead of time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Skipping for time being, came to know schemaRDD has been relaced by DatFrames from spark 1.3 onwards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sample application using SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)  # create SQLContext\n",
    "val weather = sc.textFile(\"/.../nycweather.csv\") # create RDD from a csv  \n",
    "case class Weather(date : String, temp : Int, prec : Double) # Define the schema of RDD using case class \n",
    "\n",
    "import sqlContext.implicits._  # To be able to convert RDD to a data frame\n",
    "\n",
    "val weather1 = weather.map(row => row.split(\",\")).               # Split the RDD into columns, and covert each row into a Weather \n",
    "map(row => Weather(row(0), row(1).toInt, row(2).toDouble)).      # object. Note that row of a data frame is accesses as row(i)\n",
    "toDF()                                                          # programtically for an element, convert to a DF\n",
    "\n",
    "weather1.registerTempTable(\"Wthr\")                              # Register as temp table\n",
    "val hottest = sqlContext.sql(\"select * from Wthr where prec > 0.0 order by temp desc\")  # Use queryies on table to yield an RDD\n",
    "\n",
    "hottest.map(row => (\"Time:\" + row(0), \"Temp:\" + row(1), \"Prec:\" + row(2))).top(10).  # apply ops. on the resulting RDD\n",
    "foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Spark Streaming   \n",
    "Streaming data processed in batches, like batches of RDDs being processed.    \n",
    "Can take inputs from Kafka, Flume, HDFS etc.   \n",
    "Supports operations like sliding window operations.   \n",
    "\n",
    "Need to visit details at time fo need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MLlib  \n",
    "Regression  \n",
    "Classification  \n",
    "Clustering  \n",
    "Collaborative Filtering  \n",
    "Dimenstionality Reduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GraphX   \n",
    "Graph processing library for social network analysis and language modeling.   \n",
    "####  Data Paralled vs Graph Paralled   \n",
    "Graph parallel data and processing is not efficient on data parallel systems ( like spark, hadoop map reduce).  \n",
    "Need to learn more about it later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
