{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage\n",
    "1. Writing a file from local file system to hadoop\n",
    "2. Starting Spark session\n",
    "3. Creating RDDs \n",
    "  - RDD data types\n",
    "  - Reading from local file system or hdfs (not working in jupyter)\n",
    "  - From other RDDs using transformations\n",
    "    - Using python functional programming style to create RDD transformations (map, filter, reduce) \n",
    "    - map, reduce, filter in python\n",
    "4. Checking Lineage (DebugtoString)  \n",
    "5. Memory Management\n",
    "6. Additional transformations on RDDs based on types\n",
    "  - paired RDDs\n",
    "  - Other useful transformations\n",
    "7. Paired RDD Operations and case \n",
    "8. Memory Management : Persistence, Caching, Serialization  \n",
    "9. Optimization in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Writing a file from local file system to hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd\n",
    "! touch purple_cow.txt\n",
    "!ls\n",
    "with open('purple_cow.txt', 'w') as con:\n",
    "    con.write(\"\"\"I've never seen a purple cow\n",
    "    I never hope to see one;\n",
    "    But, i can tell you, anyhow,\n",
    "    I'd rather see than be one\"\"\")\n",
    "\n",
    "!cat purple_cow.txt\n",
    "#! hadoop fs -ls fractalUS\n",
    "! hadoop fs -put purple_cow.txt fractalUS/\n",
    "!hadoop fs -cat fractalUS/purple_cow.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Starting Spark session and accessing spark context, which is used to create and access RDDs\n",
    "\n",
    "spark 2.3.1 use Spark Session for entry point instead of spark context\n",
    "Spark Session can be use use to create sql context (for Data Frames) as well as hivcontext, spark context\n",
    "for RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('rdds').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "#from pyspark import SparkContext\n",
    "#sc = SparkContext(appName= 'rdds')\n",
    "#rdd = sc.textFile('/home/sumad/purple_cow.txt')\n",
    "#rdd.take(2)\n",
    "\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Creating RDDs\n",
    "- Types of RDDs\n",
    "- from files, collections etc \n",
    "- Create RDDs using common transformations - map, filter, reduce \n",
    "  - Use of functional programming : \n",
    "    - pass function to a function, and use of anonymous functions\n",
    "    - rdd methods support functional programming, i.e they take functions as input and apply them \n",
    "    over each line in data\n",
    "- See the DAG of transformations \n",
    "- DAGs:\n",
    "   - provide lazy evaluations \n",
    "   - start computation from point of failure \n",
    "   - all transformations created RDDs in memory (unlike write to disk in map reduce)\n",
    "- Row wise transformations where possible   \n",
    "  - Where possible transformations happen row by row, i.e a row is taken through all transformations to an \n",
    "  action, instead of all blocks being acted by a single transformation at once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Types of RDD \n",
    "Can hold any type of element\n",
    "- Primitive type( integerm character etc)\n",
    "- Sequence RDDs ( from dics, lists, tuples)\n",
    "- Mixed data dype\n",
    "- Pair RDDs ( support special transformations)\n",
    "- Double RDDs (support numeric transformations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Creating RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from python collections\n",
    "sc.parallelize([1,2,3,4])\n",
    "\n",
    "# From text files \n",
    "## textFile only works with line delimited files\n",
    "## Each line in text file is a new record in RDD\n",
    "sc.textFile('dir/*.log') # all files in the dir with .log\n",
    "\n",
    "# Xml or json files\n",
    "## As there is no new line delimiter\n",
    "sc.wholeTextFiles(dir) # reads whol file as RDD with whole file as a single element, size should be checked\n",
    "\n",
    "Other input and Output formats available \n",
    "https://spark.apache.org/docs/2.3.1/api/python/pyspark.html#pyspark.RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 From transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### map, filter, reduce in python \n",
    "- find sum of squares of first 10 integers that are even\n",
    "- functional programming : functions serve as i/p and o/p units, and are chained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "l = range(1,11,1)\n",
    "f = filter(lambda x: x%2 ==0, l)\n",
    "m = map(lambda x : x**2, f)\n",
    "r = reduce(lambda x,y: x+ y, m)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Convert all words in the text file into caps, and count lines that start with I, then do a word count\n",
    "\n",
    "rdd = sc.textFile('/user/sumad/fractalUS/purple_cow.txt')\n",
    "# act on each line of text file, each line is a string \n",
    "# filter needs a function to operate on each line and return boolean\n",
    "rdd2 = rdd.map(lambda x: x.upper()) \\\n",
    "   .filter(lambda x: x.strip().startswith('I')) \n",
    "rdd2.count()\n",
    "#3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using print improves readability of DAG\n",
    "## show starting from text file two RDDs are created\n",
    "print(rdd2.toDebugString())\n",
    "\"\"\"\n",
    "(2) PythonRDD[4] at RDD at PythonRDD.scala:49 []\n",
    " |  /user/sumad/fractalUS/purple_cow.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []\n",
    " |  /user/sumad/fractalUS/purple_cow.txt HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Word Count\n",
    "rdd3 = rdd.map(lambda x: x.strip()).flatMap(lambda x:x.split(' ')).map(lambda x: x.upper())\\\n",
    ".map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y)\n",
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Checking lineage \n",
    "- Can see shuffle operation because of reduceByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2) PythonRDD[16] at collect at <stdin>:1 []\n",
    " |  MapPartitionsRDD[15] at mapPartitions at PythonRDD.scala:129 []\n",
    " |  ShuffledRDD[14] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
    " +-(2) PairwiseRDD[13] at reduceByKey at <stdin>:2 []\n",
    "    |  PythonRDD[12] at reduceByKey at <stdin>:2 []\n",
    "    |  /user/sumad/fractalUS/purple_cow.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []\n",
    "    |  /user/sumad/fractalUS/purple_cow.txt HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Memory Management \n",
    "- When RDD's are created, they are created in RAM. What happens if memory is used up. \n",
    "  - RDDs are ejected by LRU (Last used rule) \n",
    "  - Persisting RDDs affects this rule, to be covered later in detail. \n",
    "  - **Also, if RDDs are not persisted, then they are cleaned from the memory after an action is called**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Additional transformations (key ones)\n",
    "- List of all\n",
    "  - https://spark.apache.org/docs/2.3.1/api/python/pyspark.html#pyspark.RDD\n",
    "\n",
    "- Operate and return single RDD\n",
    "  - flatMap : first breaks sequence inside each RDD element, then, combines all elements in a single sequence,so\n",
    "    retutns a single RDD with single element.\n",
    "  - map , filter, reduce\n",
    "  - foreach, distinct , top(n), first\n",
    "  - min, max, mean, stddev\n",
    "  - sample, randomSplit, \n",
    "- Operate on two RDDs\n",
    "  - zip, intersection, union , subtract \n",
    "  - join \n",
    "- Operate on Paired RDD \n",
    "  - countByKey\n",
    "  - groupByKey, reduceByKey,aggregateByKey : latter two perform better than first for grouping and aggregating\n",
    "- Return paired RDD \n",
    "  - countByValue : paired RDD with unique element and its count\n",
    "- Explicit repartitioning or operations by partition\n",
    "  - mapPartitions :\n",
    "  - glom : combines elements in each partition into a unique list of elements\n",
    "  - colesce : returns rdd by repartioning to specified no. of partitions \n",
    "  - foreachPartition \n",
    "  - getNumPartitions\n",
    "  - partitionBy, repartition (coalesce can prevent a shuffle operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Paired RDD operations and case study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Memory Management - Persistence, Caching, Serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Optimization in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
