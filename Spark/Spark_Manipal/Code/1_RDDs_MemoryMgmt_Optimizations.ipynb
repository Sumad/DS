{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage\n",
    "1. Writing a file from local file system to hadoop\n",
    "2. Starting Spark session\n",
    "3. Creating RDDs \n",
    "  - RDD data types\n",
    "  - Reading from local file system or hdfs (not working in jupyter)\n",
    "  - From other RDDs using transformations\n",
    "    - Using python functional programming style to create RDD transformations (map, filter, reduce) \n",
    "    - map, reduce, filter in python\n",
    "4. Checking Lineage (DebugtoString)  \n",
    "5. Memory Management\n",
    "6. Additional transformations on RDDs based on types\n",
    "  - paired RDDs\n",
    "  - Other useful transformations\n",
    "7. Paired RDD Operations and case \n",
    "8. Memory Management : Persistence, Caching, Serialization  \n",
    "9. Optimization in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Writing a file from local file system to hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd\n",
    "! touch purple_cow.txt\n",
    "!ls\n",
    "with open('purple_cow.txt', 'w') as con:\n",
    "    con.write(\"\"\"I've never seen a purple cow\n",
    "    I never hope to see one;\n",
    "    But, i can tell you, anyhow,\n",
    "    I'd rather see than be one\"\"\")\n",
    "\n",
    "!cat purple_cow.txt\n",
    "#! hadoop fs -ls fractalUS\n",
    "! hadoop fs -put purple_cow.txt fractalUS/\n",
    "!hadoop fs -cat fractalUS/purple_cow.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Starting Spark session and accessing spark context, which is used to create and access RDDs\n",
    "\n",
    "spark 2.3.1 use Spark Session for entry point instead of spark context\n",
    "Spark Session can be use use to create sql context (for Data Frames) as well as hivcontext, spark context\n",
    "for RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('rdds').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "#from pyspark import SparkContext\n",
    "#sc = SparkContext(appName= 'rdds')\n",
    "#rdd = sc.textFile('/home/sumad/purple_cow.txt')\n",
    "#rdd.take(2)\n",
    "\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Creating RDDs\n",
    "- Types of RDDs\n",
    "- from files, collections etc \n",
    "- Create RDDs using common transformations - map, filter, reduce \n",
    "  - Use of functional programming : \n",
    "    - pass function to a function, and use of anonymous functions\n",
    "    - rdd methods support functional programming, i.e they take functions as input and apply them \n",
    "    over each line in data\n",
    "- See the DAG of transformations \n",
    "- DAGs:\n",
    "   - provide lazy evaluations \n",
    "   - start computation from point of failure \n",
    "   - all transformations created RDDs in memory (unlike write to disk in map reduce)\n",
    "- Row wise transformations where possible   \n",
    "  - Where possible transformations happen row by row, i.e a row is taken through all transformations to an \n",
    "  action, instead of all blocks being acted by a single transformation at once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Types of RDD \n",
    "Can hold any type of element\n",
    "- Primitive type( integerm character etc)\n",
    "- Sequence RDDs ( from dics, lists, tuples)\n",
    "- Mixed data dype\n",
    "- Pair RDDs ( support special transformations)\n",
    "- Double RDDs (support numeric transformations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Creating RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from python collections\n",
    "sc.parallelize([1,2,3,4])\n",
    "\n",
    "# From text files \n",
    "## textFile only works with line delimited files\n",
    "## Each line in text file is a new record in RDD\n",
    "sc.textFile('dir/*.log') # all files in the dir with .log\n",
    "\n",
    "# Xml or json files\n",
    "## As there is no new line delimiter\n",
    "sc.wholeTextFiles(dir) # reads whol file as RDD with whole file as a single element, size should be checked\n",
    "\n",
    "Other input and Output formats available \n",
    "https://spark.apache.org/docs/2.3.1/api/python/pyspark.html#pyspark.RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 From transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### map, filter, reduce in python \n",
    "- find sum of squares of first 10 integers that are even\n",
    "- functional programming : functions serve as i/p and o/p units, and are chained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "l = range(1,11,1)\n",
    "f = filter(lambda x: x%2 ==0, l)\n",
    "m = map(lambda x : x**2, f)\n",
    "r = reduce(lambda x,y: x+ y, m)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Convert all words in the text file into caps, and count lines that start with I, then do a word count\n",
    "\n",
    "rdd = sc.textFile('/user/sumad/fractalUS/purple_cow.txt')\n",
    "# act on each line of text file, each line is a string \n",
    "# filter needs a function to operate on each line and return boolean\n",
    "rdd2 = rdd.map(lambda x: x.upper()) \\\n",
    "   .filter(lambda x: x.strip().startswith('I')) \n",
    "rdd2.count()\n",
    "#3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using print improves readability of DAG\n",
    "## show starting from text file two RDDs are created\n",
    "print(rdd2.toDebugString())\n",
    "\"\"\"\n",
    "(2) PythonRDD[4] at RDD at PythonRDD.scala:49 []\n",
    " |  /user/sumad/fractalUS/purple_cow.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []\n",
    " |  /user/sumad/fractalUS/purple_cow.txt HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Word Count\n",
    "rdd3 = rdd.map(lambda x: x.strip()).flatMap(lambda x:x.split(' ')).map(lambda x: x.upper())\\\n",
    ".map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y)\n",
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Checking lineage \n",
    "- Can see shuffle operation because of reduceByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2) PythonRDD[16] at collect at <stdin>:1 []\n",
    " |  MapPartitionsRDD[15] at mapPartitions at PythonRDD.scala:129 []\n",
    " |  ShuffledRDD[14] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
    " +-(2) PairwiseRDD[13] at reduceByKey at <stdin>:2 []\n",
    "    |  PythonRDD[12] at reduceByKey at <stdin>:2 []\n",
    "    |  /user/sumad/fractalUS/purple_cow.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []\n",
    "    |  /user/sumad/fractalUS/purple_cow.txt HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Memory Management \n",
    "- When RDD's are created, they are created in RAM. What happens if memory is used up. \n",
    "  - RDDs are ejected by LRU (Least recently used) \n",
    "  - Persisting RDDs affects this rule, to be covered later in detail. \n",
    "  - **Also, if RDDs are not persisted, then they are cleaned from the memory after an action is called**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Additional transformations (key ones)\n",
    "- List of all\n",
    "  - https://spark.apache.org/docs/2.3.1/api/python/pyspark.html#pyspark.RDD\n",
    "\n",
    "- Operate and return single RDD\n",
    "  - flatMap : first breaks sequence inside each RDD element, then, combines all elements in a single sequence,so\n",
    "    retutns a single RDD with single element.\n",
    "  - map , filter, reduce\n",
    "  - foreach, distinct , top(n), first\n",
    "  - min, max, mean, stddev\n",
    "  - sample, randomSplit, \n",
    "- Operate on two RDDs\n",
    "  - zip, intersection, union , subtract \n",
    "  - join \n",
    "- Operate on Paired RDD \n",
    "  - countByKey\n",
    "  - groupByKey, reduceByKey,aggregateByKey : latter two perform better than first for grouping and aggregating\n",
    "- Return paired RDD \n",
    "  - countByValue : paired RDD with unique element and its count\n",
    "- Explicit repartitioning or operations by partition\n",
    "  - mapPartitions :\n",
    "  - glom : combines elements in each partition into a unique list of elements\n",
    "  - colesce : returns rdd by repartioning to specified no. of partitions \n",
    "  - foreachPartition \n",
    "  - getNumPartitions\n",
    "  - partitionBy, repartition (coalesce can prevent a shuffle operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Paired RDD operations and case study\n",
    "- pair RDDs have k, v pairs \n",
    "- many methods available : grouping, counting, aggregating over groups , join etc\n",
    "- Common flow of operations. \n",
    "  - Creating pair RDDs\n",
    "    - map, flatMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case Study, using telecom weblogs\n",
    "1. What are the potential marketable customers for MMS product\n",
    "  - jpg, mp3, mp4 type of message\n",
    "  - Data given in log file:  ip address, phone number,  time, file type, address, file link,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_rdd = sc.textFile('/user/sumad/weblogs/*.log')\n",
    "#ip_rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[u'3.94.78.5 - 69827 [15/Sep/2013:23:58:36 +0100] \"GET /KBDOC-00033.html HTTP/1.0\" 200 14417 \n",
    " \"http://www.loudacre.com\"  \"Loudacre Mobile Browser iFruit 1\"', \n",
    " u'3.94.78.5 - 69827 [15/Sep/2013:23:58:36 +0100] \"GET /theme.css HTTP/1.0\" 200 3576 \n",
    " \"http://www.loudacre.com\"  \"Loudacre Mobile Browser iFruit 1\"']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. By customer id, day, can we get count of mms type messages sent\n",
    "# 2. Get by customer id, average mms messages per day, get top 10 \n",
    "\n",
    "extract_id_day_mms = lambda x: (x[2], x[3][1:12])\n",
    "def mms(x):\n",
    "    if(('html' in x) | ('png' in x) | ('css' in x)):\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)    \n",
    "ip_rdd2 = ip_rdd.filter(mms).map(lambda x: x.split(' ')).map(extract_id_day_mms)\n",
    "by_cust = ip_rdd2.map(lambda x: (x[0], 1)).reduceByKey(lambda x,y:x+y)\n",
    "#by_cust.take(5)\n",
    "by_cust_dt = ip_rdd2.map(lambda x: (x, 1)).reduceByKey(lambda x,y:x+y)\\\n",
    ".map(lambda x:(x[0][0],x[1])).reduceByKey(lambda x,y:x+y)\n",
    "#by_cust_dt.take(5)\n",
    "avg_per_day = by_cust.leftOuterJoin(by_cust_dt)\\\n",
    ".map(lambda x: (x[0],x[1][0]/x[1][1]))\\\n",
    ".sortBy(keyfunc = lambda x:x[1],ascending = False)\n",
    "avg_per_day.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(avg_per_day.toDebugSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Memory Management - Persistence, Caching, Serialization\n",
    "- When RDD's are created, they are created in RAM. What happens if memory is used up. \n",
    "  - RDDs are ejected by LRU (Least recently used) \n",
    "  - If RDD is persisted, then it is given priority \n",
    "- Persist vs Cache \n",
    "  - Notes\n",
    "  - 6 options with consideration for : Memory/Disk , Serialization or not, Copying partitions or not. \n",
    "    - allow fo considering available memory vs disk, conserve memory ( by serializing) but increasing \n",
    "    processing time for deserializing , replication to revover from failure if persisted rdd is lost (saves time,\n",
    "    when a long pipeline is built)\n",
    "  - Link to documentation : https://spark.apache.org/docs/2.3.0/rdd-programming-guide.html#rdd-persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel\n",
    "test = sc.parallelize([1,2,3,4,5,6])\n",
    "test1 = test.map(lambda x : x**2)\n",
    "test2 = test1.filter(lambda x : x%2 ==0)\n",
    "test2.collect()\n",
    "\n",
    "test1.persist(storageLevel = StorageLevel.MEMORY_ONLY)\n",
    "test2.collect()\n",
    "test2.collect() # this is faster, because persistence take effect first time after \n",
    "                # rdd is specified for persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Optimization in Spark\n",
    "- Stages and Tasks \n",
    "   - Stages created whenever there is partitioning \n",
    "   - Optimization is focussed on reducing stages as much as possible, involves design discussions from architects \n",
    "   - Short circuiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check, the below operation should not cause a repartition , and hence should be a stage\n",
    "extract_id_day_mms = lambda x: (x[2], (x[3][1:12],1))\n",
    "def mms(x):\n",
    "    if(('html' in x) | ('png' in x) | ('css' in x)):\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)\n",
    "    \n",
    "ip_rdd = sc.textFile('/user/sumad/weblogs/*.log')\n",
    "ip_rdd2 = ip_rdd.filter(mms).map(lambda x: x.split(' ')).map(extract_id_day_mms)\n",
    "ip_rdd2.take(5)\n",
    " print(ip_rdd2.toDebugString())\n",
    "    \n",
    "\"\"\"\n",
    "(182) PythonRDD[3] at RDD at PythonRDD.scala:49 []\n",
    "  |   /user/sumad/weblogs/*.log MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []\n",
    "  |   /user/sumad/weblogs/*.log HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stages and tasks with times can be better seen on Spark UI \n",
    "- Giving spark the full job , lets it optimize the total run time  \n",
    "- Big executions, should be given as spark submit presumably, rather than interactively, ti optimize time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "print(avg_per_day.toDebugString())\n",
    "\n",
    "(182) PythonRDD[28] at RDD at PythonRDD.scala:49 []\n",
    "  |   MapPartitionsRDD[22] at mapPartitions at PythonRDD.scala:129 []\n",
    "  |   ShuffledRDD[21] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
    "  +-(182) PairwiseRDD[20] at sortBy at <stdin>:3 []\n",
    "      |   PythonRDD[19] at sortBy at <stdin>:3 []\n",
    "      |   PartitionerAwareUnionRDD[16] at union at NativeMethodAccessorImpl.java:0 []\n",
    "      |   PythonRDD[14] at RDD at PythonRDD.scala:49 []\n",
    "      |   MapPartitionsRDD[5] at mapPartitions at PythonRDD.scala:129 []\n",
    "      |   ShuffledRDD[4] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
    "      +-(182) PairwiseRDD[3] at reduceByKey at <stdin>:1 []\n",
    "          |   PythonRDD[2] at reduceByKey at <stdin>:1 []\n",
    "          |   /user/sumad/weblogs/*.log MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []\n",
    "          |   /user/sumad/weblogs/*.log HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []\n",
    "      |   PythonRDD[15] at RDD at PythonRDD.scala:49 []\n",
    "      |   MapPartitionsRDD[13] at mapPartitions at PythonRDD.scala:129 []\n",
    "      |   ShuffledRDD[12] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
    "      +-(182) PairwiseRDD[11] at reduceByKey at <stdin>:2 []\n",
    "          |   PythonRDD[10] at reduceByKey at <stdin>:2 []\n",
    "          |   MapPartitionsRDD[9] at mapPartitions at PythonRDD.scala:129 []\n",
    "          |   ShuffledRDD[8] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
    "          +-(182) PairwiseRDD[7] at reduceByKey at <stdin>:1 []\n",
    "              |   PythonRDD[6] at reduceByKey at <stdin>:1 []\n",
    "              |   /user/sumad/weblogs/*.log MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []\n",
    "              |   /user/sumad/weblogs/*.log HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []\n",
    "              \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
