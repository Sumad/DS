{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Churn for High Net individuals on prepaid using usage based churn \n",
    "\n",
    "# imports \n",
    "from pyspark.sql import SparkSession\n",
    "rom pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('sumad_test').getOrCreate()\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "#from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib notebook \n",
    "plt.style.use('bmh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "#### Read Data, Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### had to pull data to local file system and then read\n",
    "\n",
    "df_raw = spark.read.format(\"csv\")\\\n",
    " .option(\"header\", \"true\")\\\n",
    " .option(\"inferSchema\", \"true\")\\\n",
    " .load(\"/user/sumad/telecom_churn_data.csv\")\\\n",
    " .coalesce(5)\n",
    " #.where(\"Description IS NOT NULL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.count()\n",
    "\n",
    "len(df_raw.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_raw.limit(5).toPandas()\n",
    "\n",
    "df_sample.to_csv('sample_telco.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter high networth individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_1 = df_raw.withColumn('avg_rech_months_6_7', \n",
    "                             (F.col('total_rech_amt_6') + F.col('total_rech_amt_7'))/2)\n",
    "th_amt = df_raw_1.select('avg_rech_months_6_7').summary('70%').collect()\n",
    "\n",
    "df_raw_2 = (df_raw_1.withColumn('HNI_Ind',F.when(F.col('avg_rech_months_6_7') >= float(th_amt[0][1]),1)\n",
    "                                .otherwise(0)).filter(F.col('HNI_Ind') == 1))\n",
    "df_raw_2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create churn indicator\n",
    "\n",
    "df_raw_3 = df_raw_2.withColumn('Churn', F.when(((F.col(\"total_ic_mou_9\") == 0) &\n",
    "(F.col(\"total_og_mou_9\") == 0) &\n",
    "(F.col(\"vol_2g_mb_9\") == 0) &\n",
    "(F.col(\"vol_3g_mb_9\") == 0)),1).otherwise(0))\n",
    "\n",
    "df_raw_3.groupBy('Churn').count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Churn rate of 9% in overall data can be seen\n",
    "\n",
    "#### Drop all month 9 variables\n",
    "\n",
    "month_9_cols = [x for x in df_raw_3.columns if(x.endswith('9'))]\n",
    "print(len(df_raw_3.columns))\n",
    "print(len(month_9_cols))\n",
    "\n",
    "cols_to_keep_1 = [x for x in df_raw_3.columns if(x not in month_9_cols)]\n",
    "\n",
    "df_raw_4 = df_raw_3.select(cols_to_keep_1)\n",
    "\n",
    "len(df_raw_4.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Data Quality Check\n",
    "\n",
    "id_cols = ['mobile_number', 'circle_id' ]\n",
    "target_col = 'Churn'\n",
    "cat_cols = [x for x in df_raw_4.columns if(('date' in x) & (x not in id_cols) & (x!=target_col))]\n",
    "num_cols = [x for x in df_raw_4.columns if x not in (cat_cols + id_cols + [target_col])]\n",
    "\n",
    "\n",
    "print(len(cat_cols))\n",
    "print(len(num_cols))\n",
    "\n",
    "num_summary = df_raw_4.select(num_cols).summary().toPandas()\n",
    "\n",
    "num_summary.to_csv('num_summary.csv')\n",
    "\n",
    "cat_summary = df_raw_4.select(cat_cols).summary().toPandas()\n",
    "\n",
    "cat_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix data issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Remove numerical variables that \n",
    "- have any missing values \n",
    "- have 0 standard deviation \n",
    "#### Remove all date columns\n",
    "\n",
    "num_summary.head()\n",
    "\n",
    "num_summary_ = num_summary.set_index('summary')\n",
    "\n",
    "num_summary_.head()\n",
    "\n",
    "mask = (num_summary_.loc['stddev'] != '0.0') | (num_summary_.loc['count'] == '30011')\n",
    "\n",
    "sum(mask)\n",
    "\n",
    "num_cols_sub = list(num_summary_.columns[mask].values)\n",
    "\n",
    "#num_cols_sub\n",
    "\n",
    "def drop_null_columns(df):\n",
    "    \"\"\"\n",
    "    This function drops all columns which contain null values.\n",
    "    :param df: A PySpark DataFrame\n",
    "    \"\"\"\n",
    "    null_counts = df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]).collect()[0].asDict()\n",
    "    to_drop = [k for k, v in null_counts.items() if v > 0]\n",
    "    df = df.drop(*to_drop)\n",
    "    return df\n",
    "\n",
    "# Drops column b2, because it contains null values\n",
    "final_df = drop_null_columns(df_raw_4)\n",
    "\n",
    "len(final_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.withColumn('arpu_6_7', \n",
    "                             (F.col('arpu_6') + F.col('arpu_7')))\n",
    "final_features_1 = [x for x in (final_df.columns ) if ((x not in (id_cols + [target_col])) & \n",
    "                                                     (x in num_cols_sub))]\n",
    "len(final_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Split into training and test set \n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "#final_features_1 = [x for x in final_features if (x not in (id_cols + [target_col]))]\n",
    "\n",
    "#final_features_1\n",
    "\n",
    "assembler = VectorAssembler(inputCols= final_features,outputCol=\"features\")\n",
    "output = assembler.transform(df_raw_4).select('features', 'Churn')\n",
    "output.show(5)\n",
    "train_data,test_data = output.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 3. Model Training  \n",
    "- Model categories to be considered  \n",
    "- Handling class imbalance \n",
    "- Parameter tuning using Cross Validation , choice of eval metric\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "dtc_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction',\n",
    "                                       labelCol='Churn')\n",
    "reg_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction',\n",
    "                                       labelCol='Churn')\n",
    "\n",
    "\n",
    "log_reg = LogisticRegression(featuresCol='features',labelCol='Churn')\n",
    "\n",
    "paramGrid_reg = ParamGridBuilder()\\\n",
    "    .addGrid(log_reg.regParam, [0.1, 0.01]) \\\n",
    "    .build()\n",
    "\n",
    "crossval_reg = CrossValidator(estimator=log_reg,\n",
    "                          estimatorParamMaps=paramGrid_reg,\n",
    "                          evaluator=reg_eval,\n",
    "                          numFolds=5, parallelism = 2)\n",
    "cvModel_reg = crossval_reg.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier(labelCol='Churn',featuresCol='features')\n",
    "\n",
    "#dtc.explainParams()\n",
    "\n",
    "dtc = DecisionTreeClassifier(labelCol='Churn',featuresCol='features')\n",
    "\n",
    "paramGrid_dtc = ParamGridBuilder()\\\n",
    "    .addGrid(dtc.maxDepth, [2,5])\\\n",
    "    .build()\n",
    "\n",
    "crossval_dtc = CrossValidator(estimator=dtc,\n",
    "                          estimatorParamMaps=paramGrid_dtc,\n",
    "                          evaluator=dtc_eval,\n",
    "                          numFolds=5, parallelism = 2)\n",
    "cvModel_dtc = crossval_dtc.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 4. Model Evaluation on test\n",
    "- Use a single metric \n",
    "- Visualize where which model is better\n",
    "\n",
    "result_dtc = cvModel_dtc.transform(test_data)\n",
    "result_reg = cvModel_reg.transform(test_data)\n",
    "\n",
    "dtc_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction',\n",
    "                                       labelCol='Churn')\n",
    "reg_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction',\n",
    "                                       labelCol='Churn')\n",
    "\n",
    "print(AUC_reg, AUC_dtc) = SparkSession.builder.appName('sumad_test').getOrCreate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
