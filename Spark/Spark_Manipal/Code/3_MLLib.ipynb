{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics\n",
    "1. ML and MLLib libraries \n",
    "  - Data frame vs RDD based , latter getting deprecated\n",
    "2. Matrix support \n",
    "  - Sparse and Dense vectors and matrices \n",
    "3. Working with libSVM kind of data. \n",
    "4. Feature Transformers  \n",
    "5. Feature Extractors   \n",
    "6. Feature Selectors \n",
    "7. Model selection and Tuning \n",
    "8. Pipelines\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Linear Algebra Module \n",
    "- Dense vectors, matrices   \n",
    "  - Dense and Sparse are interconvertible, Dense rep. is essentially same as numpy array \n",
    "- Sparse vectors, matrices \n",
    "  - Can take in scipy.sparse type matrices ( check how to create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn as sk\n",
    "\n",
    "from pyspark.ml.linalg import SparseMatrix, DenseMatrix, Vectors\n",
    "\n",
    "x = Vectors.dense(np.arange(1,20,1))\n",
    "x\n",
    "print(type(x))\n",
    "\n",
    "x.dot(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import SparseVector\n",
    "a = SparseVector(4, [1, 3], [3.0, 4.0])\n",
    "a\n",
    "\n",
    "# conversion of dense matri to sparse or numpy array\n",
    "x = DenseMatrix(5,6, range(30))\n",
    "x\n",
    "\n",
    "x.toSparse()\n",
    "\n",
    "x.toArray()\n",
    "\n",
    "# sparse matrix types in scipy - two of ones that allow inverse calculation\n",
    "from scipy.sparse import csc_matrix, csr_matrix \n",
    "\n",
    "A = csr_matrix([[1, 2, 0], [0, 0, 3], [4, 0, 5]])\n",
    "A\n",
    "\n",
    "print(A)\n",
    "\n",
    "B = csc_matrix([[1, 2, 0], [0, 0, 3], [4, 0, 5]])\n",
    "B\n",
    "\n",
    "print(B)\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "denseVec = Vectors.dense(1.0, 2.0, 3.0)\n",
    "size = 3\n",
    "idx = [1, 2] # locations of non-zero elements in vector\n",
    "values = [2.0, 3.0]\n",
    "sparseVec = Vectors.sparse(size, idx, values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Working with libSVM kind of data.  \n",
    "- libsvm is a popular data format for large scale ML esp, used in popular SVM libraries LIBSVM and LIBLINEAR. \n",
    "- Format is - \n",
    "  - label index1:value1 index2:value2 ...  \n",
    "  - index is 1 based, post reading made to 0 based in spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "libsvm = spark.read.format('libsvm').load('/user/sumad/Data/sample_libsvm_data.txt')\n",
    "libsvm.show(5)\n",
    "+-----+--------------------+\n",
    "|label|            features|\n",
    "+-----+--------------------+\n",
    "|  0.0|(692,[127,128,129...|\n",
    "|  1.0|(692,[158,159,160...|\n",
    "|  1.0|(692,[124,125,126...|\n",
    "|  1.0|(692,[152,153,154...|\n",
    "|  1.0|(692,[151,152,153...|\n",
    "+-----+--------------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Feature transformers \n",
    "- Ref : https://spark.apache.org/docs/2.3.0/ml-features.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Categorical feature encoding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1.1 String Indexer  \n",
    "- Creates a numeric index on a categorical column, essentially assigning frequenct based index to each \n",
    "  category  \n",
    "- When fit on a new data set, three ways to handle new categories encountered : error, remove records, assign \n",
    "  a new index to all new cats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_ind = StringIndexer(inputCol= 'category', outputCol = 'catIndex',\n",
    "handleInvalid='error', stringOrderType='frequencyDesc')\n",
    "df_indexed = s_ind.fit(df).transform(df)\n",
    "df_indexed.show()\n",
    "\n",
    "+---+--------+--------+\n",
    "| id|category|catIndex|\n",
    "+---+--------+--------+\n",
    "|  0|       a|     0.0|\n",
    "|  1|       b|     2.0|\n",
    "|  2|       c|     1.0|\n",
    "|  3|       a|     0.0|\n",
    "|  4|       a|     0.0|\n",
    "|  5|       c|     1.0|\n",
    "+---+--------+--------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1.2 One hot encoding \n",
    "- Unlike sklearn's one hot encoder, as default, it uses a feature vector of length n-1 to represent all categories of a column . dropLast = True ensure n-1 feature vec length\n",
    "- Treatment of new category is as in StringIndexer. \n",
    "- The ouput representation is a sparse vector  \n",
    "- **When dropLast = True, any new category is thus assigned all 0 vector**  \n",
    "- OneHotEncoderEstimator can transform multiple columns, returning an one-hot-encoded output vector column for each input column. It is common to merge these vectors into a single feature vector using VectorAssembler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "df = spark.createDataFrame([\n",
    "    (0.0, 1.0),\n",
    "    (1.0, 9.0),\n",
    "    (2.0, 1.0),\n",
    "    (3.0, 2.0),\n",
    "    (0.0, 1.0),\n",
    "    (2.0, 4.0)\n",
    "], [\"categoryIndex1\", \"categoryIndex2\"])\n",
    "\n",
    "oh_enc = OneHotEncoderEstimator(inputCols= [\"categoryIndex1\", \"categoryIndex2\"],\n",
    "                      outputCols= [\"oh_categoryIndex1\", \"oh_categoryIndex2\"],\n",
    "                      handleInvalid='error',dropLast=True)\n",
    "df_enc = oh_enc.fit(df).transform(df)\n",
    "\n",
    "+--------------+--------------+-----------------+-----------------+\n",
    "|categoryIndex1|categoryIndex2|oh_categoryIndex1|oh_categoryIndex2|\n",
    "+--------------+--------------+-----------------+-----------------+\n",
    "|           0.0|           1.0|    (3,[0],[1.0])|    (9,[1],[1.0])|\n",
    "|           1.0|           9.0|    (3,[1],[1.0])|        (9,[],[])|\n",
    "|           2.0|           1.0|    (3,[2],[1.0])|    (9,[1],[1.0])|\n",
    "|           3.0|           2.0|        (3,[],[])|    (9,[2],[1.0])|\n",
    "|           0.0|           1.0|    (3,[0],[1.0])|    (9,[1],[1.0])|\n",
    "|           2.0|           4.0|    (3,[2],[1.0])|    (9,[4],[1.0])|\n",
    "+--------------+--------------+-----------------+-----------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IndextoString\n",
    "Response Encoding\n",
    "\n",
    "Inreraction ( gives a cartesian product, how is it used)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 2 VectorIndexer (Improves performance, automates cat. identification of columns)  \n",
    "  - **Operates on columns of types vectors, which means columns were concatenated to form a vector columns**  \n",
    "  - selects categorical vectors from continious based on set threshold on levels \n",
    "  - Indexing is performed UNLIKE StringIndexer, it is based on ordered values, not ordered frequencies\n",
    "  - ***Not reliable, gives weird results below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> df = spark.createDataFrame([(Vectors.dense([-1.0, 0.0, 2.0]),),(Vectors.dense([0.0, 1.0, 1.0]),), (Vectors.dense([-1.0, 1.0, 2.0]),)], [\"a\"])\n",
    ">>> indexer = VectorIndexer(maxCategories=3, inputCol=\"a\", outputCol=\"indexed\")\n",
    ">>> model = indexer.fit(df)\n",
    ">>> df.show()\n",
    "+--------------+\n",
    "|             a|\n",
    "+--------------+\n",
    "|[-1.0,0.0,2.0]|\n",
    "| [0.0,1.0,1.0]|\n",
    "|[-1.0,1.0,2.0]|\n",
    "+--------------+\n",
    "\n",
    ">>> model.categoryMaps\n",
    "{0: {0.0: 0, -1.0: 1}, 1: {0.0: 0, 1.0: 1}, 2: {1.0: 0, 2.0: 1}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector Assembler \n",
    "- Bsically concatenates columns of types numeric, vector, boolean to create a single column of vector type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from pyspark.ml.feature import VectorAssembler\n",
    ">>> \n",
    ">>> dataset = spark.createDataFrame(\n",
    "...     [(0, 18, 1.0, Vectors.dense([0.0, 10.0, 0.5]), 1.0)],\n",
    "...     [\"id\", \"hour\", \"mobile\", \"userFeatures\", \"clicked\"])\n",
    ">>> \n",
    ">>> assembler = VectorAssembler(\n",
    "...     inputCols=[\"hour\", \"mobile\", \"userFeatures\"],\n",
    "...     outputCol=\"features\")\n",
    ">>> \n",
    ">>> output = assembler.transform(dataset)\n",
    ">>> dataset.show()                                                              \n",
    "+---+----+------+--------------+-------+\n",
    "| id|hour|mobile|  userFeatures|clicked|\n",
    "+---+----+------+--------------+-------+\n",
    "|  0|  18|   1.0|[0.0,10.0,0.5]|    1.0|\n",
    "+---+----+------+--------------+-------+\n",
    "\n",
    ">>> output.show()\n",
    "+---+----+------+--------------+-------+--------------------+\n",
    "| id|hour|mobile|  userFeatures|clicked|            features|\n",
    "+---+----+------+--------------+-------+--------------------+\n",
    "|  0|  18|   1.0|[0.0,10.0,0.5]|    1.0|[18.0,1.0,0.0,10....|\n",
    "+---+----+------+--------------+-------+--------------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4.2 Continuous Feature Transformers \n",
    "- Scalers \n",
    "- Bucketizer, like cut in pandas \n",
    "- QuantileDiscretizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizer \n",
    "- transforms a dataset of vector rows, applying a p norm to each row  \n",
    "- **It is a transformer, not an estimator; unlike other scalers like Standard Scaler which need to be fit on data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=1.0)\n",
    ">>> l1NormData = normalizer.transform(dataFrame)\n",
    ">>> print(\"Normalized using L^1 norm\")\n",
    "Normalized using L^1 norm\n",
    ">>> l1NormData.show()\n",
    "+---+--------------+------------------+                                         \n",
    "| id|      features|      normFeatures|\n",
    "+---+--------------+------------------+\n",
    "|  0|[1.0,0.5,-1.0]|    [0.4,0.2,-0.4]|\n",
    "|  1| [2.0,1.0,1.0]|   [0.5,0.25,0.25]|\n",
    "|  2|[4.0,10.0,2.0]|[0.25,0.625,0.125]|\n",
    "+---+--------------+------------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard Scaler \n",
    "- Acts on a vector column, but unlike Normalizer, normalizes across column elements , using std and mean (optional) \n",
    "- If std is 0, returns 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from pyspark.ml.feature import StandardScaler\n",
    ">>> scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",\n",
    "...                         withStd=True, withMean=False)\n",
    ">>> scalerModel = scaler.fit(dataFrame)\n",
    ">>>                                                                             \n",
    ">>> # Normalize each feature to have unit standard deviation.\n",
    "... scaledData = scalerModel.transform(dataFrame)\n",
    "\n",
    ">>> scaledData.show(3, False)\n",
    "+---+--------------+------------------------------------------------------------+\n",
    "|id |features      |scaledFeatures                                              |\n",
    "+---+--------------+------------------------------------------------------------+\n",
    "|0  |[1.0,0.5,-1.0]|[0.6546536707079772,0.09352195295828244,-0.6546536707079771]|\n",
    "|1  |[2.0,1.0,1.0] |[1.3093073414159544,0.1870439059165649,0.6546536707079771]  |\n",
    "|2  |[4.0,10.0,2.0]|[2.618614682831909,1.870439059165649,1.3093073414159542]    |\n",
    "+---+--------------+------------------------------------------------------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MinMaxScalrer\n",
    "- Transforms within a given range \n",
    "MaxAbsScaler  \n",
    "- Transforms between -1 and 1, does not destroy sparsity of data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3 PolynomialExpansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> from pyspark.ml.feature import PolynomialExpansion\n",
    ">>> from pyspark.ml.linalg import Vectors\n",
    ">>> \n",
    ">>> df = spark.createDataFrame([\n",
    "...     (Vectors.dense([2.0, 1.0]),),\n",
    "...     (Vectors.dense([0.0, 0.0]),),\n",
    "...     (Vectors.dense([3.0, -1.0]),)\n",
    "... ], [\"features\"])\n",
    ">>> \n",
    ">>> polyExpansion = PolynomialExpansion(degree=3, inputCol=\"features\", outputCol=\"polyFeatures\")\n",
    ">>> polyDF = polyExpansion.transform(df)\n",
    ">>> \n",
    ">>> polyDF.show(truncate=False)\n",
    "+----------+------------------------------------------+                         \n",
    "|features  |polyFeatures                              |\n",
    "+----------+------------------------------------------+\n",
    "|[2.0,1.0] |[2.0,4.0,8.0,1.0,2.0,4.0,1.0,2.0,1.0]     |\n",
    "|[0.0,0.0] |[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]     |\n",
    "|[3.0,-1.0]|[3.0,9.0,27.0,-1.0,-3.0,-9.0,1.0,3.0,-1.0]|\n",
    "+----------+------------------------------------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipelines \n",
    "- Like sci-lit learn, concept of transformer, estimator \n",
    "- model, once fit goes from becoming an estimaor to a transformer \n",
    "- Every estimatir has a fit() method to learn params from data, and transformer had transform() \n",
    "- predict() is replaced by transform()\n",
    "- Important attributes of estimator \n",
    "  - explainParams() and extractParamMap() post fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal Example \n",
    ">>> # Prepare training data from a list of (label, features) tuples.\n",
    "... training = spark.createDataFrame([\n",
    "...     (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n",
    "...     (0.0, Vectors.dense([2.0, 1.0, -1.0])),\n",
    "...     (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n",
    "...     (1.0, Vectors.dense([0.0, 1.2, -0.5]))], [\"label\", \"features\"])\n",
    "\n",
    ">>> training.show(10)\n",
    "+-----+--------------+                                                          \n",
    "|label|      features|\n",
    "+-----+--------------+\n",
    "|  1.0| [0.0,1.1,0.1]|\n",
    "|  0.0|[2.0,1.0,-1.0]|\n",
    "|  0.0| [2.0,1.3,1.0]|\n",
    "|  1.0|[0.0,1.2,-0.5]|\n",
    "+-----+--------------+\n",
    "\n",
    "# parameters can be specified using .paramname on an estimator\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "paramMap = {lr.regParam: 0.1, lr.threshold: 0.55}\n",
    "model1 = lr.fit(training, paramMap)\n",
    "\n",
    "prediction = model2.transform(test)\n",
    "\n",
    ">>> prediction.show()\n",
    "+-----+--------------+--------------------+--------------------+----------+     \n",
    "|label|      features|       rawPrediction|       myProbability|prediction|\n",
    "+-----+--------------+--------------------+--------------------+----------+\n",
    "|  1.0|[-1.0,1.5,1.3]|[-2.8119038522838...|[0.05668429360932...|       1.0|\n",
    "|  0.0|[3.0,2.0,-0.1]|[2.48711787928029...|[0.92323378664212...|       0.0|\n",
    "|  1.0|[0.0,2.2,-1.5]|[-2.0865940788370...|[0.11040665017928...|       1.0|\n",
    "+-----+--------------+--------------------+--------------------+----------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Pipeline Example \n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "\n",
    "\n",
    ">>> tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    ">>> hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    ">>> lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    ">>> pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    ">>> model = pipeline.fit(training)\n",
    "prediction = model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ML models work on features combined together in a single column which of data type vector "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Tuning \n",
    "- Cross Validation strategy \n",
    "  - CrossValidator \n",
    "  - TrainValidationSplit.  \n",
    "- Building a parameter grid \n",
    "  - ParamGridBuilder \n",
    "  - **By default evaluation is done sequentially, however, parallilim can be set as > 1 in CrossValidator or TrainValidationSplit, which i think creates separate partitions, ideally  < 10**\n",
    "- Using Eval metrics available from Evaluator Module, each is a class\n",
    "  - RegressionEvaluator \n",
    "  - BinaryClassificationEvaluator\n",
    "  - MulticlassClassificationEvaluator \n",
    "  - **Default evaluator in estimator can be over ridden by setting setMetricName** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "+---+----------------+-----+                                                    \n",
    "| id|            text|label|\n",
    "+---+----------------+-----+\n",
    "|  0| a b c d e spark|  1.0|\n",
    "|  1|             b d|  0.0|\n",
    "|  2|     spark f g h|  1.0|\n",
    "|  3|hadoop mapreduce|  0.0|\n",
    "|  4|     b spark who|  1.0|\n",
    "|  5|         g d a y|  0.0|\n",
    "|  6|       spark fly|  1.0|\n",
    "|  7|   was mapreduce|  0.0|\n",
    "|  8| e spark program|  1.0|\n",
    "|  9|         a e c l|  0.0|\n",
    "| 10|   spark compile|  1.0|\n",
    "| 11| hadoop software|  0.0|\n",
    "+---+----------------+-----+\n",
    "\n",
    ">>> from pyspark.ml import Pipeline\n",
    ">>> from pyspark.ml.classification import LogisticRegression\n",
    ">>> from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    ">>> from pyspark.ml.feature import HashingTF, Tokenizer\n",
    ">>> from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=2, parallelism = 2)\n",
    "cvModel = crossval.fit(training)\n",
    "prediction = cvModel.transform(test)\n",
    "+---+---------------+------------------+--------------------+--------------------+--------------------+----------+\n",
    "| id|           text|             words|            features|       rawPrediction|         probability|prediction|\n",
    "+---+---------------+------------------+--------------------+--------------------+--------------------+----------+\n",
    "|  4|    spark i j k|  [spark, i, j, k]|(1000,[105,149,32...|[-1.0143531895130...|[0.26612878920913...|       1.0|\n",
    "|  5|          l m n|         [l, m, n]|(1000,[6,638,655]...|[2.45505377427970...|[0.92093023893998...|       0.0|\n",
    "|  6|mapreduce spark|[mapreduce, spark]|(1000,[105,953],[...|[-0.2292614916964...|[0.44293435984699...|       1.0|\n",
    "|  7|  apache hadoop|  [apache, hadoop]|(1000,[181,495],[...|[1.80181132002375...|[0.85836928288627...|       0.0|\n",
    "+---+---------------+------------------+--------------------+--------------------+--------------------+----------+"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
