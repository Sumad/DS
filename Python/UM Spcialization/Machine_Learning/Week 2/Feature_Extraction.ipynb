{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction \n",
    "Module in sklearn to extract features from text as well as images. \n",
    "1. Text Feature extraction\n",
    "    - BOW based representation, dimensionality problem\n",
    "      - CountVectorizer\n",
    "      - TfIdfTransformer\n",
    "      - TfIdfVectorizer\n",
    "      - Customizing Vectorizers\n",
    "    - Limitations of BOW and Hashing tricks  \n",
    "      - FeatureHashing\n",
    "    - Loading features from dictionary   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 BOW / Bag of n-word based representation \n",
    "- Text corpus represented in terms of a dictionary of words \n",
    "- Each document in corpus then represented as a numerical vector, using the BOW\n",
    "- The vector can comprise of counts, normalized counts, TfIDf etc\n",
    "- **Limitation of the approach is that BOW representation does not capture the context**  \n",
    "- General steps for vectorizing a document \n",
    "  - Vocabulary creation \n",
    "    - tokenize, i.e split sentences into 1-grams, 2 gramss etc.\n",
    "    - stemming (working -> work), lemmatization (bad -> good), again need to think about the application \n",
    "      in the context\n",
    "    - remove stop words, most applications of vector representation will not benefit from these often used words\n",
    "    - Finally assign an index to each token in the BOW\n",
    "  - Document term matrix  \n",
    "    - Columns are indices of words in BOW. \n",
    "    - Rows are documents / sentences \n",
    "    - data is count / normalized count / tfidf etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 CountVectorizer  \n",
    "Vectorizer classes convert documents to DTM i.e vectorize documents to numerical features. \n",
    "- sklearn class implements \n",
    "  - tokenization (n-grams) : default setting to use punctuation and white space to identify token boundaries  \n",
    "  - stop word removal : uses a standard english stop word list, not best for every use case\n",
    "  - vocab creation : assigning index to each word in the bag \n",
    "    - **Be aware of traps : we've splits to 'we' and 've', stop word may remove we, not ve**\n",
    "  - Vocabs can be be of hugee dimension, DTM is created as a sparse scipy matrix  \n",
    "  - DTM is normalized across rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer : performs pre-processing on text, use of arguments\n",
    "- strip_accents : remove accents like above and convert to corresponding unocode characters\n",
    "- preprocessor :  by default, just remove accents using a funcion or an option specified by strip_accents\n",
    "- tokenizer : works with other arguments, to produce tokens\n",
    "  - analyzer : word / char / char_wb (i.e how should tokens be created, from words or chatracters)\n",
    "    - char_wb is word boundary aware, can be useful to overcome mis-spellings, insteasd of char \n",
    "    - **n-gram can be good to preserve some location specific information if to be used as features**\n",
    "    - **Wider NLP tasks that aim to extract structures from sentences are outside scope of sklearn**\n",
    "  - ngram_range : ngrams based on words or characters as specified above\n",
    "  - uses a default regex pattern to tokenize by considering whitespace or punctuation as boundaries, and ignores boundaries, when creating tokens, also takes token of min length 2 \n",
    "- stop_words : \n",
    "  - shortcomings of stop word lists should be understood before applying \n",
    "  - paper : http://aclweb.org/anthology/W18-2502 \n",
    "    - stop words should ideally be taken through same pre-processing and tokenization scheme\n",
    "    - can be generated by studying intra-corpus term frequency\n",
    "- token_pattern : used only if tokenizer is word, pattern uses space and punctiation to mark boundaries\n",
    "- ngram_range : tuple, with min and max range of n-grams\n",
    "- min_df , max_df : useful to truncate vocab , by setting lower and upper limit on document frequency. \n",
    "- max_features : max n features, ordered by frequency. \n",
    "- vocabulary : supplied vocab\n",
    "- binary :  usefule to generate a binary value, instead of count "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### accent removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and',\n",
       " 'are',\n",
       " 'document',\n",
       " 'first',\n",
       " 'is',\n",
       " 'nt',\n",
       " 'one',\n",
       " 'second',\n",
       " 'the',\n",
       " 'these',\n",
       " 'third',\n",
       " 'this',\n",
       " 'åa']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "     'These are`nt åa the first document~',\n",
    "     'This is the second second document.',\n",
    "     'And the third one.',\n",
    "     'Is this the first document?']\n",
    "count_vect = CountVectorizer(strip_accents = None)\n",
    "count_vect.fit_transform(corpus)\n",
    "count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa',\n",
       " 'and',\n",
       " 'are',\n",
       " 'document',\n",
       " 'first',\n",
       " 'is',\n",
       " 'nt',\n",
       " 'one',\n",
       " 'second',\n",
       " 'the',\n",
       " 'these',\n",
       " 'third',\n",
       " 'this']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\n",
    "     'These are`nt åa the first document~',\n",
    "     'This is the second second document.',\n",
    "     'And the third one.',\n",
    "     'Is this the first document?']\n",
    "count_vect = CountVectorizer(strip_accents = 'unicode')\n",
    "count_vect.fit_transform(corpus)\n",
    "count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### using 1-gram and 2-gram token to captures locality context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': 0,\n",
       " 'first': 1,\n",
       " 'first document': 2,\n",
       " 'is': 3,\n",
       " 'is this': 4,\n",
       " 'the': 5,\n",
       " 'the first': 6,\n",
       " 'this': 7,\n",
       " 'this the': 8}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\n",
    "     ['Is this the first document?']\n",
    "count_vect = CountVectorizer(strip_accents = 'unicode',analyzer= 'word',ngram_range= (1,2))\n",
    "X = count_vect.fit_transform(corpus)\n",
    "count_vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'these', 'third', 'this']\n",
      "{'these': 7, 'is': 3, 'the': 6, 'first': 2, 'document': 1, 'this': 9, 'second': 5, 'and': 0, 'third': 8, 'one': 4}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 1, 1, 0, 0],\n",
       "       [0, 1, 0, 1, 0, 2, 1, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 1, 0, 1, 0, 1, 0],\n",
       "       [0, 1, 1, 1, 0, 0, 1, 0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\n",
    "     'These is the first document~',\n",
    "     'This is the second second document.',\n",
    "     'And the third one.',\n",
    "     'Is this the first document?']\n",
    "count_vect = CountVectorizer()\n",
    "X = count_vect.fit_transform(corpus)\n",
    "print(count_vect.get_feature_names())\n",
    "print(count_vect.vocabulary_)\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 TfidfTransformer \n",
    "- Only using counts, will make features that have high occurence across documents important \n",
    "- rescale count/frequency, by factoring how frequent the word is across documents helps\n",
    "  achive balance betweem term frequeny, and document frequency of term. \n",
    "- $tfidf(t,d) = tf(t,d) * idf(t)$\n",
    "- $idf(t) = log(\\frac{n_d}{1+df(t)})$ , textbook definition, range of the function can go from -inf to inf. \n",
    "- sklearn implement smoothed and non-smoothed versions, and has an option to normalize across rows/documents \n",
    "  - non-smoothed\n",
    "  $idf(t) = 1 + log( \\frac{n_d}{df(t)})$ , this is always >=1, but can have 0 division on new data\n",
    "  - smoothed\n",
    "  $idf(t) = 1 + log( \\frac{1 + n_d}{1+df(t)})$, cannot have 0 divisions on new data  \n",
    "- sublinear tf = 1 + log(tf).   \n",
    "- **in case of short length texts, tf-idf can be noisy, binary vectorization is more appropriate**\n",
    "- **pipeline feature when building models, can allow for tuning all these as hyperparamters to a model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.38782252,  0.47903796,  0.38782252,  0.        ,\n",
       "         0.        ,  0.31707032,  0.60759891,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.26714448,  0.        ,  0.26714448,  0.        ,\n",
       "         0.8370669 ,  0.21840812,  0.        ,  0.        ,  0.32997658],\n",
       "       [ 0.55280532,  0.        ,  0.        ,  0.        ,  0.55280532,\n",
       "         0.        ,  0.28847675,  0.        ,  0.55280532,  0.        ],\n",
       "       [ 0.        ,  0.41812662,  0.51646957,  0.41812662,  0.        ,\n",
       "         0.        ,  0.34184591,  0.        ,  0.        ,  0.51646957]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer(norm = 'l2', smooth_idf= True, sublinear_tf= False)\n",
    "Y = tfidf.fit_transform(X)\n",
    "Y.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### reproduce the first vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.38782252,  0.47903796,  0.38782252,  0.        ,\n",
       "        0.        ,  0.31707032,  0.60759891,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "v = [0, 1 + np.log(5/4), 1 + np.log(5/3), 1 + np.log(5/4), 0,0,1 + np.log(5/5),1 + np.log(5/2),0,0]\n",
    "v/norm(v, ord = 2, keepdims= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3 TfidfVectorizer \n",
    "- basically combines, CountVectorizer and TfIdfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Uses, Limitations of BOW  representation \n",
    "Uses \n",
    "- Classificaton of text documents. \n",
    "- Information retreival applications, also involve Clustering documents   \n",
    "- Topic modeling ( using LDA, NMF)  \n",
    "\n",
    "Shortcomings -\n",
    "- Do not capture context that well, somewhat improvement possible with character , n-grams \n",
    "- **When scoring on new dataset, can encounter new words that were not in vocabulary, in applications like \n",
    "spam classification, that can be a way to avert a classifier**  \n",
    "- **BOW vocab has to be stored in RAM, and can get complex to to store big vocabularies**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.4 Customizing Vectorizers \n",
    "- preprocessor, tokenizer, analyzer can be customized to work with other processing tasks like stemming, lemmatization\n",
    "etc. by NLTK \n",
    "- Examples covered in the documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Feature Hashing \n",
    "- Idea of feature hashing is to not construct a BOW or vocab at all, rather define a hash function, that takes\n",
    "let us say words of a sentence as input, and generates a fixed length vector representation of the sentence/document\n",
    "- Feature hashing concepts are explained here - https://medium.com/value-stream-design/introducing-one-of-the-best-hacks-in-machine-learning-the-hashing-trick-bf6a9c8af18f  \n",
    "  - No inverse transform\n",
    "  - collisisons, selecting a large feature space to avoid collisions\n",
    "- sklearn implements a FeatureHasher class and a Vectorizer   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0., -4., -1.,  0.,  0.,  0.,  0.,  0.,  2.],\n",
       "       [ 0.,  0.,  0., -2., -5.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = FeatureHasher(n_features=10)\n",
    "D = [{'dog': 1, 'cat':2, 'elephant':4},{'dog': 2, 'run': 5}]\n",
    "f = h.transform(D)\n",
    "f.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### HashingVectorizer first vectorizes and extract toakens and their counts ( like the D above) and then projects into the n-feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "HashingVectorizer(n_features= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        , -0.87287156, -0.21821789,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.43643578],\n",
       "       [ 0.        ,  0.        ,  0.        , -0.37139068, -0.92847669,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = ['dog cat cat elephant elephant elephant elephant',\n",
    "    'dog dog run run run run run']\n",
    "hv = HashingVectorizer(n_features= 10)\n",
    "X = hv.fit_transform(c)\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dimenstionality is not a constraint when the matrix is a CSR matrix, and algorithms that work with CSR\n",
    "LinearSVC(dual=True), Perceptron, SGDClassifier, PassiveAggressive,\n",
    "but it does for algorithms that work with CSC matrices (LinearSVC(dual=False), Lasso(), etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Out of core learning with Hashing \n",
    "- Example https://scikit-learn.org/stable/auto_examples/applications/plot_out_of_core_classification.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 DictVectorizer \n",
    "- Takes features in form of a dictionary, which offers a concise way of bundling features, instead of in a dataframe\n",
    "- **sometime features extracted for sequence models are in dictionary format (TBD)**\n",
    "- This vectorizer converts the categorical features in the dictionary to a one hot encoded form \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.,   0.,   0.,  33.],\n",
       "       [  0.,   1.,   0.,  12.],\n",
       "       [  0.,   0.,   1.,  18.]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> measurements = [\n",
    "...     {'city': 'Dubai', 'temperature': 33.},\n",
    "...     {'city': 'London', 'temperature': 12.},\n",
    "...     {'city': 'San Francisco', 'temperature': 18.},\n",
    "... ]\n",
    "\n",
    ">>> from sklearn.feature_extraction import DictVectorizer\n",
    ">>> vec = DictVectorizer()\n",
    "\n",
    ">>> vec.fit_transform(measurements).toarray()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
