{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import nltk\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch_20newsgroups?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = fetch_20newsgroups(subset='train', shuffle=True,\n",
    "                              random_state=42, remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = raw_data.data\n",
    "target = raw_data.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comp.windows.x'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\nOf course.  The term must be rigidly defined in any bill.\\n\\n\\nI doubt she uses this term for that.  You are using a quote allegedly\\nfrom her, can you back it up?\\n\\n\\n\\n\\nI read the article as presenting first an argument about weapons of mass\\ndestruction (as commonly understood) and then switching to other topics.\\nThe first point evidently was to show that not all weapons should be\\nallowed, and then the later analysis was, given this understanding, to\\nconsider another class.\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Pre-process data and create a BOW dictionary  \n",
    "- Remove digits,there are a lot if them,  punctuations, replace with space(nltk)\n",
    "- remove short words or letters \n",
    "- lemmatize(nltk)\n",
    "- Tokenize using white space, remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "#import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to wordnet...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords', 'wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()\n",
    "#punc = string.punctuation\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_series = pd.Series(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_1 = docs_series.str.replace('[^a-zA-Z]', ' ')\n",
    "documents_2 = documents_1.apply(lambda doc : ' '.join([wrd.lower() for wrd in doc.split() if len(wrd) >3]))\n",
    "documents_3 = documents_2.apply(lambda doc : ' '.join([lemma.lemmatize(word) for word in doc.split()]))\n",
    "documents_4 = documents_3.apply(lambda doc : ' '.join([word for word in doc.split() if word not in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Do you have Weitek's address/phone number?  I'd like to get some information\n",
      "about this chip.\n",
      " \n",
      "  Do you have Weitek s address phone number   I d like to get some information about this chip   \n",
      " have weitek address phone number like some information about this chip \n",
      " have weitek address phone number like some information about this chip \n",
      " weitek address phone number like information chip\n"
     ]
    }
   ],
   "source": [
    "print(documents[3],'\\n',documents_1[3], '\\n',documents_2[3],'\\n',documents_3[3],'\\n',documents_4[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemma.lemmatize('corpora')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### min_df, max_df affect vocab formation, max_features filters after vocab is formed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. From BOW, create a TF-IDF document representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_tfidf = TfidfVectorizer(lowercase=True, analyzer='word',\n",
    "                        ngram_range=(1,1), min_df=1,max_df=0.9,max_features=1000,\n",
    "                        use_idf=True, norm='l2', smooth_idf=True, sublinear_tf=False)\n",
    "dtm_1 = vect_tfidf.fit_transform(documents_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 1000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vect_tfidf.get_feature_names()\n",
    "#temp.vocabulary_['800']\n",
    "#max(sorted(vect_tfidf.vocabulary_.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TfidfVectorizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. On the matrix, apply a truncated SVD \n",
    "- inspect document-topic matrix U \n",
    "- S matrix \n",
    "- topic-term matrix i.e Vt, and guess topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=20, n_iter=100,\n",
       "       random_state=42, tol=0.0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=100,\n",
    "            random_state=42)\n",
    "svd.fit(dtm_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1000)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 : ['stephanopoulos' 'char' 'jpeg' 'null' 'azerbaijani' 'stream' 'remark']\n",
      "Topic 1 : ['people' 'year' 'think' 'team' 'right' 'game' 'christian']\n",
      "Topic 2 : ['people' 'file' 'christian' 'window' 'jesus' 'government' 'would']\n",
      "Topic 3 : ['game' 'window' 'thanks' 'file' 'anyone' 'team' 'know']\n",
      "Topic 4 : ['thanks' 'please' 'anyone' 'know' 'doe' 'mail' 'would']\n",
      "Topic 5 : ['file' 'please' 'mail' 'space' 'system' 'email' 'address']\n",
      "Topic 6 : ['card' 'chip' 'driver' 'video' 'system' 'monitor' 'sale']\n",
      "Topic 7 : ['people' 'card' 'christian' 'doe' 'please' 'armenian' 'jesus']\n",
      "Topic 8 : ['would' 'window' 'please' 'card' 'jesus' 'christian' 'mail']\n",
      "Topic 9 : ['would' 'game' 'card' 'chip' 'system' 'file' 'team']\n",
      "Topic 10 : ['window' 'right' 'armenian' 'thanks' 'drive' 'government' 'year']\n",
      "Topic 11 : ['window' 'chip' 'game' 'system' 'christian' 'jesus' 'doe']\n",
      "Topic 12 : ['like' 'window' 'people' 'right' 'game' 'chip' 'government']\n",
      "Topic 13 : ['doe' 'year' 'would' 'anyone' 'window' 'sale' 'price']\n",
      "Topic 14 : ['think' 'year' 'good' 'driver' 'team' 'window' 'player']\n",
      "Topic 15 : ['bike' 'right' 'file' 'game' 'would' 'good' 'work']\n",
      "Topic 16 : ['chip' 'part' 'driver' 'like' 'time' 'could' 'bike']\n",
      "Topic 17 : ['game' 'system' 'space' 'bike' 'time' 'card' 'nasa']\n",
      "Topic 18 : ['year' 'chip' 'people' 'armenian' 'last' 'jesus' 'game']\n",
      "Topic 19 : ['think' 'part' 'game' 'year' 'driver' 'please' 'anyone']\n"
     ]
    }
   ],
   "source": [
    "terms = np.array(vect_tfidf.get_feature_names())\n",
    "for i in range(svd.components_.shape[0]):\n",
    "    arr = svd.components_[i, :]\n",
    "    print('Topic {0} : {1}'.format(i, terms[np.argsort(arr)[0:7]]))\n",
    "#svd.components_[0,0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20,)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd.singular_values_.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ability', 'accept'], dtype='<U14')"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms_arr[[0,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72916"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Evaluate how orthogonal the topics are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
