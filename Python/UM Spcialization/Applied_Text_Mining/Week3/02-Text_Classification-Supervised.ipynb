{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries for text classification problems\n",
    "### 1. Scikit-learn  \n",
    "### 2. nltk \n",
    "Interacts with Weka and other ML libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Scikit-learn  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use of scikit-learn\n",
    "## Naive bayes\n",
    "from sklearn import naive_bayes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Multinomial Naive Bayes or bernoulli naive bayes  \n",
    "#### Multinomial NB object created  \n",
    "NB_clfr = naive_bayes.MultinomialNB()  \n",
    "#### Call method with train date and label of target\n",
    "NB_clfr.fit(train_data, train_labels)  \n",
    "#### Predit method  \n",
    "NB_clfr.predict(test_data)  \n",
    "#### Validation on test_data using a metric of choice  \n",
    "metrics.f1score(test_labels, predicted_labels, 'micro')  \n",
    "\n",
    "##### Averaging type can be micro and macro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKlearn's svm classifier\n",
    "from sklearn import svm\n",
    "\n",
    "#### build an SVM object\n",
    "svm_clfr = svm.SVC(kernel= 'linear', C = 0.1)  \n",
    "###### default kernel is rbf and c is 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the object to call methods to train and predict\n",
    "svm_clfr.fit(train_data, train_labels)  \n",
    "svm_clfr.predict(test_data)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection \n",
    "svm has tuning parameters like cv. Model selection can be done using **repeated** k-fold cv.  \n",
    "k-fold cv helps one to utilie all data for training, and also compare model, no validation/tuning data needs to be cut out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn import model_selection  \n",
    "predict_cv = model_selection.cross_val_predict(X= train_date,y= train_labels,cv= 5,method= svm_clfr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. nltk   \n",
    "nltk has own classifiers  \n",
    "- NaiveBayesClassifier  \n",
    "- DecisionTreeClassifier  \n",
    "- ConditionalExponentialClassifier  \n",
    "- Maxent Classifier (Max Entropy)  \n",
    "\n",
    "It also allows to call sklearn and weka's classifiers  \n",
    "- WekaClassifier  \n",
    "- SklearnClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from nltk import NaiveBayesClassifier\n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "#### Get labels on test set\n",
    "classifier.classify(score_set)\n",
    "\n",
    "#### For multi-label classification\n",
    "classifier.classify_many(score_set)\n",
    "\n",
    "#### Generate metrics after scoring\n",
    "nltk.classify.util.accuracy(classifier, test_set)\n",
    "\n",
    "#### Method to get variable importance, pass the no. of features to see\n",
    "\n",
    "classifier.show_most_informative_features()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For svm , there is no native svm function, but can use scikitlearn's\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Sentiment Analysis \n",
    "\n",
    "#### Data \n",
    "Amazon Reviews, datset structure:   \n",
    "1. Product Name   \n",
    "2. Price  \n",
    "3. Brand  \n",
    "4. Rating  \n",
    "5. Review Votes  \n",
    "6. Review Comments  \n",
    "\n",
    "#### Task : Identify Positive or Negative sentiment of the product by using review comments  \n",
    "Rating >3 as positive  , other as negative  label  \n",
    "\n",
    "#### 1. Split Data into train, test for now\n",
    "\n",
    "#### 2. Feature Engineering - Features can be made using \n",
    "1. Word count  \n",
    "2. TF-IDF, but we first need to form a bag of words/vocabulary that identifies with positive or negative sentiment.  \n",
    "3. Can even make n-grams as features, but that will bulk up the feature space  \n",
    "4. Other custom ways  \n",
    "\n",
    "#### a. CountVectorizer could help in building a bag of words/vocabulary first. It does the following steps - \n",
    "1. tokenizes documents into words, recognizing **sentence boundaries**  \n",
    "2. Performs normalization of words, converting them to lower case    \n",
    "3. performs stemming and lemmatization   \n",
    "4. Prepare a matrix, where each column name is one of the tokens, row is the document, data is the count of occurence of the   \n",
    "   word. This matrix is understandably sparse, with lots of 0's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Fit the CountVectorizer to the training data, this gives the bag of words\n",
    "vect = CountVectorizer().fit(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2b. Build a document term matrix from the bag of words. It does the following steps -  \n",
    "\n",
    "transform the documents in the training data to a document-term matrix  \n",
    "X_train_vectorized = vect.transform(X_train)  \n",
    "X_train_vectorized   \n",
    "\n",
    "** This will be a sparse matrix, with a whole lot of features**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Fit a model using a method  \n",
    "**Use Logistic Regression first**   \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression  \n",
    "\n",
    "Train the model  \n",
    "model = LogisticRegression()  \n",
    "model.fit(X_train_vectorized, y_train)  \n",
    "\n",
    "**Generate predictions, after transformating test set into DTM as well**  \n",
    "\n",
    "from sklearn.metrics import roc_auc_score  \n",
    "\n",
    "**Predict the transformed test documents**  \n",
    "predictions = model.predict(vect.transform(X_test))  \n",
    "\n",
    "** Any words in test set, that do not appear in train set will just be ignored **\n",
    "\n",
    "**Evaluate Predictions**  \n",
    "\n",
    "print('AUC: ', roc_auc_score(y_test, predictions))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Observe the features with largest and smallest coefficients  \n",
    "**get the feature names as numpy array**   \n",
    "feature_names = np.array(vect.get_feature_names())  \n",
    "\n",
    "**Sort the coefficients from the model**  \n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "**Find the 10 smallest and 10 largest coefficients  \n",
    "The 10 largest coefficients are being indexed using [:-11:-1]   \n",
    " so the list returned is in order of largest to smallest**  \n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))  \n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering with TF IDF \n",
    "TF is term frequence i.e   \n",
    "no. of time a wrod occurs in a doc / total words in the doc  \n",
    "IDF is a weight, giving weight to the words that are comparatively rare in the corpus of socuments.   \n",
    "IDF = log(1/ (no. of documents having the word / total documents))  \n",
    "as log is an increasing positiv function beyond 1, the weight increases as word is rare   \n",
    "\n",
    "**So, TF-IDF can be expected to be high for words that occur very times in the corupus, but are often repeated in some documents. It will weigh down the words repeated too often like stop words to 0**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can derive TF-IDF DTM matrix using TFidfVectorizer. \n",
    "\n",
    "** TFidf will give same features as count based matrix, but sparser. So, we can ensure to keep only   \n",
    "those features that occur a certain no. of times in the frequency matrix. The same argument can be used in TF DTM as well.  \n",
    "it ensures words that occur in a minimum no. of documents, become part of the library**\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#### Fit the TfidfVectorizer to the training data specifiying a minimum document frequency of 5\n",
    "vect = TfidfVectorizer(min_df=5).fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering with n-grams\n",
    "We get context in words by using 2-grams or 3-grams at times, rather than just using single words. Example, in review, you could  \n",
    "have words like - 'not a fan' , 'not excited' ; which are not getting captured to carry a negative sentiment.  \n",
    "\n",
    "**Fit the CountVectorizer to the training data specifiying a minimum \n",
    " document frequency of 5 and extracting 1-grams and 2-grams**  \n",
    " \n",
    "** Having the min_df argument, really helps to reduce features that could be just noice and not a pattern**\n",
    "\n",
    "vect = CountVectorizer(min_df=5, ngram_range=(1,2)).fit(X_train)\n",
    "\n",
    "X_train_vectorized = vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** The sklearn vectorizers are very adaptive  - read documentation**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
