{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supervised Learning for Text / Text Classification\n",
    "Examples :  \n",
    "*A. Text Classification at document level, document could be paragraph/sentence/ documents*\n",
    "1. Topic Identification  : Looking at documents, classify b/w types - say Entertainment, Sports, Politics in news articles  \n",
    "2. Spam Detection : emails are spam/non spam, binary classification  \n",
    "3. Sentiment Analysis : Classify movie reviews on imdb as positive or negative or neutral  \n",
    "*B. Word level*  \n",
    "1. Spelling correction problem while typing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High Level structure  \n",
    "1. Classes or Labels for prediction  : Could be 2 or multiple classes  \n",
    "2. Features/Properties : What will be useful features, what pre-processing needs to be done to build features  \n",
    "3. Methods/Algortithms and Assessment Metric, \n",
    "4. Training data - use for training, parameter tuning, use CV \n",
    "5. Test - Benchmark, compare models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify Features  \n",
    "Context is most important, but types of features - \n",
    "1. Words minus stop words  \n",
    "- not normalized : white house or White House   \n",
    "- not Lemmatized : spam could have bad english  \n",
    "- based on POS, and using a pattern of grammar \n",
    "- grouping words of similar meaning together   \n",
    "- other patterns : dates, digits\n",
    "2. bi-gramms, tri-grams, unstemmed words\n",
    "\n",
    "We basically use the Week1 and Week 2 pre-processing ways to derive features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive bayes Classifier\n",
    "$P(X|Y) = \\frac{P(Y|X) * P(X)}{P(Y)}$  \n",
    "\n",
    "##### Example of classifying a document into 1 of the three classes, \n",
    "say we try to do it based on 1 feature, i.e word 'Python'. Classes are - entertainment, CS, zoology  \n",
    "\n",
    "- Prior Probability or Prior Belief is : probability of finding Ent., CS, zoology documents  \n",
    "- Posterior Probability : Improvement to prior based on new knowledge, that of word 'Pyhton'  \n",
    "- Likelihood is the probability of finding 'Python' in each of these documents, it is conditional probability  \n",
    "- Evidence : Probability of finding 'Python' in all types of documents   \n",
    "\n",
    "So, posterior = (prior prob. X prob. of finding evidence given prior) / prob. of finding evidence  \n",
    "\n",
    "#### Naive Bayes Classifier  is a probabilistic model\n",
    "Find the three posterior probabilities, and whichever is greatest, label the class as that  \n",
    "P(doc = \"Ent\" | feature = \"Python\") = P(doc = \"Ent\") * P(feature = \"Python\" | doc = \"Ent\" ) / P(feature = \"Python\")  \n",
    "P(doc = \"CS\" | feature = \"Python\") = P(doc = \"CS\") * P(feature = \"Python\" | doc = \"CS\" ) / P(feature = \"Python\")  \n",
    "P(doc = \"Zoology\" | feature = \"Python\") = P(doc = \"Zoology\") * P(feature = \"Python\" | doc = \"Zoology\" ) / P(feature = \"Python\")  \n",
    "**P(feature = \"Python\") is same, so we just focus on Prior and Likelihood.**   \n",
    "\n",
    "For multiple features, we get the following formulation of Naive Bayes, with  \n",
    "**Naive Assumption**  \n",
    "The features are independent of each other, i.e probability of finding one feature does not influence the prob.  of  \n",
    "other features.  \n",
    "\n",
    "**Naive Bayes Formulation** \n",
    "\n",
    "$y* =  argmax { P(doc = <class>) * \\pi \\frac{P(feature = x_i)} {P(doc = <class>) }}$  \n",
    "\n",
    "** Features **  \n",
    "\n",
    "P(x_i|y) = Relative count of featured x_i in documents marked class 'y' \n",
    "P(y)   \n",
    "\n",
    "** Smoothing **  \n",
    "What if P(x_i|y) = 0   \n",
    "\n",
    "We do additive smooting or laplace smoothing -   so add 1 to all counts of features, so   \n",
    "P(x_i|y) = (n_i + 1)/(n_t + p) , where p is no. of parameters.  \n",
    "\n",
    "Key Concepts:  \n",
    "1. Naive Bayes' assumption fails often for text, as features are not truly independent  \n",
    "2. it is a good , sinple method to get a first estimate out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two classic types of Naive Bayes Models    \n",
    "**Bernoulli Model**\n",
    "- You make a bag of words, and make features as absence/presence of words.  \n",
    "- The count of workds in the documents does not matter  \n",
    "  Example: spam classifier, you can beleive that spam emails would follow this distribution   \n",
    "**Multinomial Model**  \n",
    "- The features are either count of words (from the bag of words) of TF-IDF i.e Term Frequency * Inverse Document Frequency  \n",
    "  of words. Example: the count could be important relative to its count in the entire corpus of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Classifier   for binary classification \n",
    "\n",
    "**SVM is not a probabilistic model, it directly gives classification**\n",
    "\n",
    "1. Decision Boundaries  \n",
    "2. Overfitting vs Underfitting  \n",
    " a. Complex non-linear boundary : Overfit   \n",
    " b. Linear Boundary : Underfit  \n",
    "3. SVM Classifiers  \n",
    "SVM classifiers are linear classifiers that find a hyperplane to separate two classes of data.   \n",
    "**Support Vectors**  \n",
    "Support vectors are hyperplanes derived using the concept of maximum margins, with the classification boundary is at the centre of these hyperplanes   \n",
    "\n",
    "**Maximum Margin Hyperplane**  \n",
    "a.We learn two margins such that, these margins are parallel to each other, and  the classifier is at the centre of them.   \n",
    "b. If a small change in data points is made, there is a minimum change in the class labels. The margins are based using the support of multiple points and are called support vectors.  \n",
    "\n",
    "**How do we learn them**  \n",
    "Given we have a single feature as x_i = {x1, x2, x3, .... xn}  \n",
    "The classification hyperplane is found as  f(x_i) = <w.x_i> + b, where w is the weight vector to be found, and b is a bias term.  \n",
    "The classification rule is -  if (f(x_i) >= 0, then class_1, else class_0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Class Classification \n",
    "**One vs Rest Approach**   \n",
    "So, if there are n classes in the data set, you have n classifiers.\n",
    "**One vs Other Approach**  \n",
    "nC2 , for 3 classes, there are 3 classifiers  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters in SVM  \n",
    "- Cost (c) is a parameter  for penalizing wrong classification\n",
    "    - Smaller values allow for more tolerance to errors, gives simpler models, default values i 1\n",
    "- Kernels: linear, rbf, polynomial  \n",
    "    - linear kernels work best for text data\n",
    "- multi class : over(one vs rest preferred)\n",
    "- class weight: \n",
    "    -  Weighting classes, in case of skewed distriutions. Give more weight to smaller class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crucial Points and Takeaways   \n",
    "1. SVMs give very good results on text data, as they tend to work best in high dimensioonal data  \n",
    "2. Strong foundation in optimization throry, should read more on it  \n",
    "3. pre-processing is required   \n",
    "a. Convert categoricals to numeric   \n",
    "b. Normalize  \n",
    "4. Hard to interpret model, use when explainability is not a core criteria for method selection\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
