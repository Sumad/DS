{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "- Regression - Predictive Technique For Continuous Dependent Variables, Parametric\n",
    "- Classification - For Discrete Outcome, Parametric And Non Parametric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Linear Regression As A Technique - Formulation and Assumptions\n",
    "\n",
    "-   Establish a linear relationship to explain variation in a random variable using other variables:\n",
    "    regression is a method to better explain the variation in a variable, using a set of variables called\n",
    "    independent variables. with this relationship, we hope to predict the future values of the variable,\n",
    "    better than a prediction of mean on an average. better prediction means that the average error in prediction on an     unseen data will be less than that produced by using preiction as a mean value of variable at hand.\n",
    "   - Formulation : $\\widehat{y} =  \\widehat{\\beta_0} + \\widehat{\\beta_1}*x_1 + ... + \\widehat{\\beta_1}*x_n + \\epsilon $\n",
    "\n",
    "- We hypothesize a relationship as above we beleive applies to the population. taking expected value\n",
    "  operator on both sides, and with the below assumptions we get \n",
    "   - $E\\frac{\\widehat{y}}{X_i} =  \\widehat{\\beta_0} + \\widehat{\\beta_1}*x_1 + ... + \\widehat{\\beta_1}*x_n  $\n",
    "\n",
    "- Regression to mean:The depenent variable is the mean predicted value, and not the actual value.\n",
    "    \n",
    "**Summary and Assumptions**  \n",
    "Reference : http://localhost:8888/edit/Documents/Analytics/IIM%20Business%20Analytics%20Course/M3/Basic%20Econometrics%20-%20Personal%20Notes.doc\n",
    "\n",
    "1.\tRegressand is linearly related to the regressors\n",
    "2.\tRegressors are non-stochastic variables, or if they are stochaistic they are independent of errors i.e covariance is 0.\n",
    "3.\tExpected value of errors for a given X is 0.\n",
    "4.\tHomoscedasticity\n",
    "5.\tNo Autocorrelation\n",
    "6.\tNo perfect Multicollinearity\n",
    "7.\tNormal Distribution of errors\n",
    "8.\tSample size is more than the no. of regressors\n",
    "9.\tThe value of regressors varies, and is not constant.\n",
    "10.\tModel is correctly specified\n",
    "\n",
    "The problems can be categorized as –\n",
    "1.\tProblems w.r.t residuals and model specification\n",
    "2.\tThe problems w.r.t data : Assumptions 6,8,9 along with problems of Outliers and influential observations \n",
    "\n",
    "The assumptions which are not given much importance and reasons thereof – \n",
    "\n",
    "Ass. 2: If x is stochastic variable, then we need to specify it’s probability distribution along with that of error. \n",
    "As long as we have the data given for X variables, and they are not really stochaisitc, we are good; even if they are stochastic, if have x and error independent, we are good.\n",
    "\n",
    "Ass3: Even of Expected value of errors at a given x are not 0, and a constant value, that means we get a biased value of B1. B1 is not a significant term in regression analysis, so we can live with it\n",
    "\n",
    "Ass7 : Even if error terms is non normal, the estimators are still BLUE estimators. We required assumption of normality for drawing inferences about B’s, using (n-k)s2/sigma2 as having chi sq. dist to draw inference about pop. Regression error,  and performing hypothesis testing.\n",
    "Even if errors were not to be normal, the distribution of Bs can be approximated to normal for very large samples ( CLT).\n",
    "The normality condition must however be checked for finite samples, using normality tests like Anderson Darling,Jarque Bera test.\n",
    "\n",
    "Caution: There is no single answer that could solve all problems, it sounds better to first identify problems, their severity and then try attack them with measures that can address most of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Hypothesized relationship, prf and srf\n",
    "\n",
    "we work with a sample of data, so we hypothesize the relationship with variables in population\n",
    "but estimate the parameters from a sample.\n",
    "\n",
    "how eda plays a role in formulating the hypothesis:\n",
    "- data exploration from the sample is used to understand the following characteristics of the data\n",
    "1. continuous variables:\n",
    "a. univariate distributions - skewed, symmetrical and potential outliers\n",
    "b. bivariate distributions - linear relationships between response and predictors that can be modeled or\n",
    "non-linear relationships, which need to be approximated as linear or require a different modeling approach\n",
    "2. categorical variables:\n",
    "a. distribution of response variable in each category of the categorical variable, does mean of response\n",
    "increase linearly, or are too many categories that need consolidation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Example with a dataset \n",
    "- TBD : convert the analysis to python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crim - per capita crime rate by town\n",
    "# zn - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "# indus - proportion of non-retail business acres per town.\n",
    "# chas - charles river dummy variable (1 if tract bounds river; 0 otherwise)\n",
    "# nox - nitric oxides concentration (parts per 10 million)\n",
    "# rm - average number of rooms per dwelling\n",
    "# age - proportion of owner-occupied units built prior to 1940\n",
    "# dis - weighted distances to five boston employment centres\n",
    "# rad - index of accessibility to radial highways\n",
    "# tax - full-value property-tax rate per $10,000\n",
    "# ptratio - pupil-teacher ratio by town\n",
    "# b - 1000(bk - 0.63)^2 where bk is the proportion of blacks by town\n",
    "# lstat - % lower status of the population\n",
    "# medv - median value of owner-occupied homes in $1000's\n",
    "\n",
    "#### analytics problem is to predict house prices ####\n",
    "#### data quality checks ####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1 Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data,tag the appropriate class of each variables, specify what is missing value\n",
    "# rm(list=ls())\n",
    "dir <- \"c:/users/sumad.singh/documents/ds/supervised learning/sl201/linear regression\"\n",
    "filename <- \"boston_file.csv\"\n",
    "path <- file.path(dir,filename)\n",
    "colclasses <- c(rep(\"numeric\",3),\"integer\",rep(\"numeric\",4),\"integer\", rep(\"numeric\",5))\n",
    "data <- read.csv(file = path, header = true,\n",
    "                 na.strings = c(\"\", na),colclasses = colclasses)\n",
    "\n",
    "# consolidate the following at variable level to answer the questions\n",
    "#1. does data have missing values\n",
    "na_count <- function(x) {sum(is.na(x))}\n",
    "na.count <- apply(x = data, margin = 2,fun = na_count)\n",
    "\n",
    "#2. are there too many categories in some variables\n",
    "## find the no. of unique values in each variable\n",
    "unique_count <- function(x) {length(unique(x))}\n",
    "unique.count <- apply(x = data, margin = 2,fun = unique_count)\n",
    "\n",
    "df.quality <- data.frame(var = names(na.count), na.count = na.count, unique.count = unique.count)\n",
    "\n",
    "#### data split to train (50%), test for tuning (30%) and val for final benchmarking (20%))\n",
    "\n",
    "set.seed(101)\n",
    "data$rand <- runif(n = nrow(data),min = 0,max = 1)\n",
    "train <- data %>% filter(rand <= 0.5)\n",
    "test <- data %>% filter(rand > 0.5 & rand <= 0.8)\n",
    "val <-  data %>% filter(rand > 0.8) \n",
    "\n",
    "\n",
    "#### exploratory analysis ####\n",
    "#1. are the continuous variables skewed\n",
    "#2. do the continuous variables have outliers\n",
    "#3. do they exhibit a linear relationship with response variabes\n",
    "#4. are the predictor variables highly correlated with one another, in case of which their effect  \n",
    "# will not be separated with the estimation technique\n",
    "#5. do the ordinal categorical variables exhibit a linear relationship with mean of response\n",
    "#6. are there too many thin categories that can be clubbed together\n",
    "\n",
    "# these questions can be answered using visualization and determining some metrics\n",
    "# visualizations\n",
    "#1. plot() and scatterplotmatrix() give a good but constrained picture.try picturing non-linear,\n",
    "# relationships, splines, highly colinear relationships here.\n",
    "\n",
    "#2.a.plot each variable with response, with a regression line\n",
    "# b. draw a density plot for each variable to gauge extreme skews\n",
    "# c. draw a boxplot to gauge extent of potential outliers, and direction\n",
    "\n",
    "plot.scatter <- function(dataset, var1, response.var) {\n",
    "plot(x = dataset[,var1], y = dataset[,response.var],type = \"p\",xlab = var1, ylab = response.var, \n",
    "       main = paste(\"scatterplot for\", var1, \"with\", response.var,sep = \" \" ))\n",
    "abline(reg = lm(dataset[,response.var] ~ dataset[,var1]), lty = 2, lwd = 1.2, col = \"red\" )  \n",
    "}\n",
    "\n",
    "plot.box <- function(dataset, var1){\n",
    "  # boxplot plots, 1st and 3rd quantile, median to form hinges. whiskers are formed \n",
    "  # 1.5iqr away from the quantiles,if the whiskers are away from the min or max, whisker/s is\n",
    "  # dropped\n",
    "  # the boxplot statistics can be seen using plot = false argument, the function also gives\n",
    "  # the list of outliers (outside of 1.5 iqr), in a vector accessible as $out\n",
    "  boxplot(x = dataset[,var1],horizontal = false,\n",
    "          main = paste(\"boxplot for \", var1))\n",
    "}\n",
    "\n",
    "plot.density <-  function(dataset, var1)  {\n",
    "  # need to understand how the density values are estimated for plotting\n",
    "  densityplot(x = dataset[,var1])\n",
    "  title(main = paste(\"density plot for\", var1))\n",
    "}\n",
    "\n",
    "eda.continuous <- function(dataset, response.var, predictor.cont, dir){\n",
    "  # function to draw the plots listed on points 1 and 2 above\n",
    "#pdf(file.path(dir,\"basic_eda.pdf\"))\n",
    "  scatterplotmatrix(dataset)\n",
    "  for (var in predictor.cont){\n",
    "    layout(mat = matrix(data = c(1,1,2,3),nrow = 2,byrow = true))\n",
    "  plot.scatter(dataset, var, response.var)\n",
    "  plot.box(dataset, var)\n",
    "  plot.density(dataset, var)   \n",
    "  }\n",
    "#  dev.off()\n",
    "}\n",
    "\n",
    "cont.vars <- colnames(train)[!colnames(train) %in% c(\"rad\",\"chas\")]\n",
    "\n",
    "eda.continuous(dataset = train,response.var = \"mv\", \n",
    "               predictor.cont = cont.vars,\n",
    "               dir = dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.  metrics\n",
    "# summary with measures of central tendency and variation:\n",
    "df.summary <- apply(x = data, margin = 2, fun = summary)\n",
    "# quantiles and number of potential outliers\n",
    "# correlation matrix\n",
    "# vif\n",
    "# influential observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Observations\n",
    "#### linearity\n",
    "\n",
    "-   crim can be seen to have potential outliers (from boxplot, point more than 3iqr will be &gt;20), which are likley decreasing the slope of the line, it also has a very left skewed distribution, so linear relationship is difficult to hypothesize, unless with a transformation\n",
    "\n",
    "-   strong linear relationships are evident with rm, lstat based on the slope of the regression line, and how close the points are positioned around it\n",
    "-   medium linear relationships : crim, indus,nox, tax, pt.\n",
    "\n",
    "-   weak linear relationships :\n",
    "\n",
    "#### skew\n",
    "\n",
    "#### outliers\n",
    "\n",
    "why use an intercept in the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Validate hypotheses about relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why ols is good\n",
    "BlUE if no heteroscedasticity and autocorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.1 data selection, sampling, outliers,influential observation considerations \n",
    "#### 3.4.2 transformation of categorical variables \n",
    "- 3.4.2.1 dummy variables and dummy variable trap \n",
    "- 3.4.2.2 base level selection  \n",
    "- 3.4.2.3 identification of interactions this helps hypothesize, but whether it increase variance is tested by independent test set\n",
    "  - 3.4.2.3.1 graphical method\n",
    "  - 3.4.2.3.2 use of decision trees or chaid  \n",
    "#### 3.4.2.4 how outliers and influential observations affect estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5. parameter estimates, their std. error, p value - interpretation\n",
    "\n",
    "#### 3.5.1 how can we compare partial regression coefficients\n",
    "\n",
    "#### 3.5.2 how should difference in scale of variables be managed?\n",
    "run regression on standardized and convert to actual scales?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 validation of assumptions - how to,consequences of failure, treatment\n",
    "### 3.6.1 crucial assumptions\n",
    "- 3.6.1.1 Normality of Errors: Chisquare goodness of fit (large sample test), anderson darling, qq plot\n",
    "  - essential for reliability of f and t tests; transform skewed variables, specify model better, \n",
    "  increase sample size \n",
    "- 3.6.1.2 Homoscedasticity \n",
    "  - breusch pagan test, white test,goldfield quandt test, graphical methods of plotting stdized errors with variables (scatter plot of standardized residuals with independent variables to check funnel in/out shape, same with standardizded predicted values, often leads to transformation of dependent variable, can tie it back to skewed distribution of dependent)\n",
    "  - ols does not give best estimates,std.error would be high, treatment is weighted ols (used in glms) as ols gives\n",
    "    equal weight to all observations, also gives biased std. errors of b's (deflates) so reliability of t tests is in     question \n",
    "- 3.6.1.3 multicolinearity: perfect mc\n",
    "  - not possible to estimate coefficients high but not perfect : vif > 1, conflict in f test and t test; inflated   \n",
    "    std. errors, can reject significant vars.; drop variables, consolidate, increase sample size \n",
    "- 3.6.1.4 autocorelation:  \n",
    "  - durbin watson test, d = 2 then no ac; add variables to remove ac, use dummy vars\n",
    "\n",
    "### 3.6.2 validate hypothesis\n",
    "- 3.6.2.1 why do f test \n",
    "- 3.6.2.2 t test - individual b is 0 while others are non zero or zero, that is why\n",
    "  - under null hyp. b = 0 , i.e population parameter is o, so from multiple srfs, the mean of bs will\n",
    "    be equal to population mean i.e 0. std. error of this distribution of b, needs using standard deviation\n",
    "    of b from the same, hence use t statistic.\n",
    "- 3.6.2.3 residual tests specification bias can be picked up, like non linear relationships test for normality and homscedasticity already discussed\n",
    "\n",
    "### 3.6.3 transformations\n",
    "- necessary for heteroscedasticity, normlity, non linear relationship \n",
    "- guidelines for transformation to apply for heteroscedasticity for linearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Variable Selection\n",
    "#### 3.7.1 how to see effect of adding/ removing a variable\n",
    "\n",
    "#### 3.7.1.1 using concept of part and partial correlations\n",
    "adding a variable improves r2 equal to sq. of semi-partial correlation \n",
    "\n",
    "#### 3.7.2 how to test of variable should be added \n",
    "- USE ADJUSTED R SQUARE ESS/ TSS -> USE OF DF -> ADJ RSQ = 1- (RSS/N-K-1)/(TSS/N-1) \n",
    "  %AGE DECRREASE IN RSS SHOULD BE MORE THAN % DECREASE IN DF BY ADDING VARIABLE TO DECREASE ADJ. R2 \n",
    "#### 3.7.3 Partial f test for a bunch of variables what is the hypothesis \n",
    "- NOTE : F TEST FOR VARIABLE SELECTION RELIES ON TESTING THE HYPOTHESIS THAT A DECREASE IN RSS IS ACCOMPISHED THAT IS NOT A MATTER OF CHANCE. BUT, IT DOES NOT ACCOUNT FOR AN INCREASE IN VARIANCE BY ADDING OF THE VARIABLE IF F TEST SUGGESTS SO, TEST FOR BIAS/VARIANCE IS ENTIRELY DIFFERENT DONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8. estimate standard error of regression i.e sd of regression\n",
    "- mse i.e rss/n-k-1 is an unbiased estimator of variance of the regression \n",
    "\n",
    "### 3.9. Goodness of fit measures - limitations of rsq for selection \n",
    "### 3.10. How equation is used for prediction \n",
    "0 variance of yhat vs variance of e(y|xi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.11 Modeling strategy with regression\n",
    "- data selection\n",
    "- data quality\n",
    "- data sampling into train, test and val or when can we use cross validation - why?\n",
    "- eda - univariate, bivariate, correlations, extreme values capping\n",
    "  - again this part is meant to assist in formulating the relationship \\#\\# feature engineering based on eda, business knowledge, iterations \\#\\# variable selection strategy \\#\\#\\# traditional - forward, backward and mixed selection, shortcomings \\#\\#\\#\\# uses partial f test \\#\\#\\# new - lasso \\#\\# model fitting - understanding loss functions \\#\\# bias vs variance trade off - models performance on a test set using an assessment criteria \\#\\#\\# make multiple models to reduce spec. bias as you learn and better hypothesis\n",
    "  use cv and an assessment measure like mse, and derive the mean and sd. of the mse\n",
    "  then choose the simplest model within 1 sd. of the mse of model with lowest mse\n",
    "\n",
    "### 3.12 assessment metric to evaluate best model\n",
    "\n",
    "- aic, bic, mse, deviance\n",
    "\n",
    "### 3.13 Other strategies with regression for pricing\n",
    "\n",
    "- Adding constraints of monotonicity of coefficient for a factor variables\n",
    "\n",
    "- as no. of faults increase, the coefficient should increase \n",
    "- adding interactions to provide benefits as no. of faults increase, but so do number of cars on policy, should provide benefit so, add an interaction variable \n",
    "- deliberately keeping some categorical variables instead of continuous to avoid too thin segmentation\n",
    "- for competitive advantage age of drivers could be continuous,and the premium on policy will change at end of policy period due to increase in age if it were continuous, may not if it were categorical, and birthday lies in the\n",
    "current policy period.\n",
    "- you may want to do thin segmentatin on variables that have very high propensity for predicting risk \n",
    "- testing for discrimination using interaction can no. of females on policy be used for pricing, may be only along with no, of males? \n",
    "- using an offset to fix the base price using a separate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
