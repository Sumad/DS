{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic\n",
    "## 1. Supervised algorithms and Use Cases\n",
    "## 2. xgboost -  Learning\n",
    "2.1 linear and decision tree algorithm overview   \n",
    "  2.1.1 Linear algorithm with regularization   \n",
    "  2.1.2 Decision tree algorithm with regularization   \n",
    "  \n",
    "2.3 Objective function and Regularization in xgboost   \n",
    "2.4.Non-parametric Learning methods(Learning of rules) - Additive Training (Boosting)  \n",
    "2.5. Structure Score    \n",
    "2.6 Finding best split. \n",
    "## 3. Features of xgboost   \n",
    "3.1 Handling missing values.  \n",
    "3.2 View trees   \n",
    "3.3 Interactive feature analysis   \n",
    "3.4 Automatic sparse data handling  \n",
    "### 3.3 Extendability.  \n",
    "3.3.1 R, Python etc APIs  \n",
    "3.3.2 Extendability in defining custom objective functions  \n",
    "3.3.3 plugins - assessment metrics for regression, classification ; learners   \n",
    "3.3.3 Modular code to support more extensions ( Early stopping, Checkpointing)  \n",
    "## 4. Distributed implementation  \n",
    "4.1 Comparison with Spark and H2O implementation   \n",
    "## 5. Deployment. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Algorithms and Use Cases  \n",
    "Most common algorithm used in production systems for business are -   \n",
    "a. Linear Models    \n",
    "b. Tree Based Models.   \n",
    "c. Deep Learning Models.   \n",
    "d. Factorization Models for Recommendations. \n",
    "\n",
    "Deep Learning models work best for image, video data as they are potentially able to abstract higher order features from images and video.   \n",
    "For tabular/structured data, tree based algorithms still perform better than deep learning algorithms.  \n",
    "\n",
    "Plethora of use cases in production using tree based algorithms. - \n",
    "1. Fraud Detection. \n",
    "2. Anomaly Detection (Physics). \n",
    "3. Risk prediction. \n",
    "4. Ads clickthrough.  \n",
    "\n",
    "**Tree Based Models**   \n",
    "Tree models are better than linear models are they are able to perform better when there are non -linear relationships\n",
    "between response variable and predictors. Their advantages over linear models are - \n",
    "1. Less feature engineering as multiplicative relationships / interactions are captured   \n",
    "2. A good extent of variable selection is covered. \n",
    "3. For simple decision trees, outputs are simple rules, easily understandable   \n",
    "4. Inputs are invariant to scaling, normalization is a usual pre-processing step in parametric models to make training\n",
    "   faster.    \n",
    "\n",
    "Disadvatages:  \n",
    "1. High variance/ overfitting compared to linear models, despite use of tree pruning, regularization.   \n",
    "2. Training speed /scaling to large data sets when doing tree ensembles\n",
    "\n",
    "Tree Ensembles:  \n",
    "An ensemble of tree - Random Forest or Boosted Trees perform better than individual trees.  \n",
    "\n",
    "**Common Tree ensemble algorithms and their software implementations **  \n",
    "1. Random Forest (Breiman 1997)  \n",
    "a. R and Python. \n",
    "2. Gradient Tree Boosting (Freidman 1999)   \n",
    "a. R GBM  \n",
    "b. python sklearn GBM\n",
    "3. Gradient Tree Boosting with Regularization  \n",
    "a. Regularized Greedy Forest (RGF)  \n",
    "b. XGBoost. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. xgboost \n",
    "**Placeholder to compare training times for R GBM, Python GBM, xgboost single core as no. of trees are varied. \n",
    "Summary of what is xgboost - **\n",
    "1. Fast and Scalable implementation of Regularized GBM. \n",
    "2. Optimized from s/w engineering perspective   \n",
    "  * Cache. Optimization\n",
    "  * Distributed  \n",
    "  * Out of Core? \n",
    "  * Parallelized.   \n",
    "3. Algorithmic improvement. \n",
    "  * sparse aware. ?\n",
    "  * Weighted approx. Quantile  sketch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Linear Models and Decison Tree Models - Make up of the supervised learning problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general description of a parametric model is as below :  \n",
    "1. Model Hypothesis \n",
    "2. Model Parameters \n",
    "3. Objective Function to Optimize   \n",
    "4. Algorithm to optimize Objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For linear models :   \n",
    "* **Model Hypothesis : **  \n",
    "\n",
    "For linear regression :   \n",
    "\\begin{align}\n",
    "\\widehat y^{(i)} = \\Sigma_j  w_j * x_j^{(i)} + b  \n",
    "\\end{align}\n",
    "For logistic regression:  \n",
    "\\begin{align}\n",
    "z^{(i)} &= \\Sigma_j w_j * x_j^{(i)} + b \\\\\n",
    "\\widehat y^{(i)} &= g(z^{(i)}) \\\\\n",
    "g(z) &= 1/(1+e^{-z})\n",
    "\\end{align}  \n",
    "\n",
    "* **Model Parameters :**   \n",
    "$$w_j\\mid j = {\\{1,2,3...\\}}$$\n",
    "\n",
    "* **Objective Function to Optimize :**  \n",
    "Objective function consits of cost function and regularization. Regularization is one way of reducing the overfitting \n",
    "problem.  General form of objective function is sum of a cost function and regularization function\n",
    "$$ O(\\omega) = C (\\omega) +  \\Omega(\\omega)$$   \n",
    "\n",
    "  * **Cost functions can vary by problem type - regression, classification, ranking. Squared errors, log loss are   \n",
    "      known cost functions**  \n",
    "      \\begin{align}\n",
    "      C (\\omega) &= \\Sigma_i (y^{(i)} - \\widehat{y^{(i)}})^2 \\text{     for regression and}  \\\\\n",
    "      C (\\omega) &= \\Sigma_{i=1}^m \\frac{-1}{m}(y^{(i)}log(y^{(i)}) + (1- y^{(i)}) log(1-\\widehat y^{(i)})) \\text{    \n",
    "      for logistic regression}\n",
    "      \\end{align} \n",
    "      \n",
    "  * **Regularization used is L1 and L2 norm of the parameters**  \n",
    "      By regularizing, we try to minmiize the size of coefficients of the parametric models. Overfitting is caused           because the  more important variables in the context of learning from the training dataset, tend to have higher       coefficients. \n",
    "      Regularization, by imposing penalties, reduces the size of coefficients and hence overfitting.  \n",
    "      \n",
    "      For elastic net both forms are included as below. $\\lambda$ controls the extent of L1 and L2 penalty, $\\alpha$   \n",
    "      is the shrinkage parameter and can range from $0 - \\inf$\n",
    "        \n",
    "   \\begin{align}\n",
    "   \\Omega(\\omega) = \\alpha * [\\lambda * ||\\omega||_1 + (1 - \\lambda) * ||\\omega||_2^2 ] \\text{  elastic net}\\\\\n",
    "   ||\\omega||_1 \\text{  is L1 regulrization or Lasso penalty} \\\\\n",
    "   ||\\omega||_2^2 \\text{  is L2 regulrization or Ridge penalty} \n",
    "   \\end{align}  \n",
    "\n",
    "   **L1 or Lasso term tries to shrink the parameter matrix into a sparse matrix, and is a good choice for variable   \n",
    "   selection, even better than that offered by variable selection by tree ensembles, because it is robust against   \n",
    "   noise.  By, shrinking the weight matrix \n",
    "   into sparse matrix(i.e reducing many weight to zero), it selects the strong predictors.**     \n",
    "\n",
    "   **L2 or Ridge on other hand also reduces the size of weights, but does not bring them to zero. It does result in \n",
    "   morestable coefficients though, better than that in Lasso. It also results in collinear variables getting similar \n",
    "   coefficents, whereas as Lasso randomly selects one of the collinear variables, and reduces others to zero.**  \n",
    "\n",
    "   **So, elastic net uses both to take benefits of variable selection and stable weights. Lasso and Ridge become good \n",
    "   choices, when the features available are large in number.**\n",
    "\n",
    "* **Algorithms to optimize Objective function : **  \n",
    "\n",
    "  * Gradient Descent  \n",
    "  * Stochastic Gradient Descent  \n",
    "  * Coordinate Descent \n",
    "  and more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TBD - Variable selection comparison of RF, GBM and Lasso??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TBD - comparison of Lasso and Ridge regularization??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
